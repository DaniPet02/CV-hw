{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9f451d",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c8e4b",
   "metadata": {},
   "source": [
    "### Here is a complete, efficient PyTorch pipeline to convert all Cityscapes images and their corresponding .json label files into .pt files for later fast loading and training (e.g., for DeepLabv3+ fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Define paths ===\n",
    "IMAGE_DIR = 'images/'\n",
    "LABEL_DIR = 'labels/'\n",
    "OUTPUT_DIR = 'pt_data/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Import your label converter ===\n",
    "# Ensure this function returns a tensor of shape [7, H, W]\n",
    "from your_module import convert_label_to_multilabel_one_hot  \n",
    "\n",
    "# === Helper: convert JSON annotation to label map ===\n",
    "def parse_cityscapes_json_to_label(json_path, image_size):\n",
    "    \"\"\"\n",
    "    Converts a Cityscapes polygon annotation JSON into a 2D label map [H, W].\n",
    "    \"\"\"\n",
    "    from PIL import ImageDraw\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    label_img = Image.new('L', image_size, 0)\n",
    "    draw = ImageDraw.Draw(label_img)\n",
    "\n",
    "    for obj in data['objects']:\n",
    "        label = obj['label']\n",
    "        polygon = obj['polygon']\n",
    "        try:\n",
    "            id = int(obj['id'])  # optional, depends on how IDs are stored\n",
    "        except:\n",
    "            id = get_class_id_from_label_name(label)  # Implement this mapping\n",
    "        draw.polygon(polygon, fill=id)\n",
    "\n",
    "    return torch.from_numpy(np.array(label_img)).long()\n",
    "\n",
    "# === Main pipeline ===\n",
    "def preprocess_and_save_all():\n",
    "    image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.png')]\n",
    "\n",
    "    for img_file in tqdm(image_files, desc=\"Processing Cityscapes data\"):\n",
    "        # --- Paths\n",
    "        base_name = img_file.replace('_leftImg8bit.png', '')\n",
    "        image_path = os.path.join(IMAGE_DIR, img_file)\n",
    "        json_path = os.path.join(LABEL_DIR, base_name + '_gtFine_polygons.json')\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"Missing label for {img_file}\")\n",
    "            continue\n",
    "\n",
    "        # --- Load image and resize if needed\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0  # [3, H, W]\n",
    "\n",
    "        # --- Convert JSON to label tensor\n",
    "        label_2d = parse_cityscapes_json_to_label(json_path, image.size)  # [H, W]\n",
    "\n",
    "        # --- Convert to multilabel [7, H, W]\n",
    "        multilabel_tensor = convert_label_to_multilabel_one_hot(label_2d)\n",
    "\n",
    "        # --- Save as .pt\n",
    "        torch.save({\n",
    "            'image': image_tensor,\n",
    "            'label': multilabel_tensor\n",
    "        }, os.path.join(OUTPUT_DIR, base_name + '.pt'))\n",
    "\n",
    "preprocess_and_save_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9effdc",
   "metadata": {},
   "source": [
    "### Suggested workflow (Deprecated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8684b2",
   "metadata": {},
   "source": [
    "#### 1. Load the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20230e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "image_path = \"path/to/image.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_np = np.array(image)  # shape (H, W, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13140c",
   "metadata": {},
   "source": [
    "#### 2. Parse JSON Label\n",
    "\n",
    "Suppose each JSON contains a list of polygons with class IDs or names. Example JSON structure (pseudo):\n",
    "\n",
    "{\n",
    "  \"objects\": [\n",
    "    {\"label\": \"road\", \"polygon\": [[x1, y1], [x2, y2], ...]},\n",
    "    {\"label\": \"person\", \"polygon\": [[x1, y1], [x2, y2], ...]},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "Load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = \"path/to/label.json\"\n",
    "with open(json_path) as f:\n",
    "    label_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300580f2",
   "metadata": {},
   "source": [
    "#### 3. Initialize an Empty Label Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df68d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = image_np.shape[:2]\n",
    "label_mask = np.zeros((height, width), dtype=np.uint8)  # or int64 if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8529f",
   "metadata": {},
   "source": [
    "#### 4. Rasterize Polygons into Label Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# A mapping from label names to original Cityscapes IDs, as in your CLASS_MAPPING keys\n",
    "LABEL_NAME_TO_ID = {\n",
    "    \"road\": 7,\n",
    "    \"sidewalk\": 8,\n",
    "    \"building\": 11,\n",
    "    \"wall\": 12,\n",
    "    \"fence\": 13,\n",
    "    \"pole\": 17,\n",
    "    \"traffic sign\": 19,\n",
    "    \"traffic light\": 20,\n",
    "    \"vegetation\": 21,\n",
    "    \"terrain\": 22,\n",
    "    \"sky\": 23,\n",
    "    \"person\": 24,\n",
    "    \"rider\": 25,\n",
    "    \"car\": 26,\n",
    "    \"truck\": 27,\n",
    "    \"bus\": 28,\n",
    "    \"train\": 31,\n",
    "    \"motorcycle\": 32,\n",
    "    \"bicycle\": 33,\n",
    "}\n",
    "\n",
    "for obj in label_data[\"objects\"]:\n",
    "    label_name = obj[\"label\"]\n",
    "    polygon = np.array(obj[\"polygon\"], dtype=np.int32)\n",
    "\n",
    "    if label_name in LABEL_NAME_TO_ID:\n",
    "        class_id = LABEL_NAME_TO_ID[label_name]\n",
    "\n",
    "        # cv2.fillPoly expects a list of polygons (each polygon a numpy array)\n",
    "        cv2.fillPoly(label_mask, [polygon], class_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c424db",
   "metadata": {},
   "source": [
    "#### 5. Convert Label Mask to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = torch.from_numpy(label_mask).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c7b90",
   "metadata": {},
   "source": [
    "#### 6. Use Your Existing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_one_hot = convert_label_to_multilabel_one_hot(label_tensor)\n",
    "# multilabel_one_hot has shape [7, H, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fb42a",
   "metadata": {},
   "source": [
    "### Dataset Loader for Preprocessed Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class PrecomputedCityscapesDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir) if f.endswith(\".pt\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root_dir, self.files[idx])\n",
    "        sample = torch.load(path)\n",
    "        return sample['image'], sample['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae374e",
   "metadata": {},
   "source": [
    "## Image Preprocessing Pipeline for Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409f985",
   "metadata": {},
   "source": [
    "This script assumes the final layer of your fine-tuned DeepLabV3+ produces 7 channels (for each macro class + object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "NUM_CLASSES = 7\n",
    "CLASS_NAMES = [\"road\", \"flat\", \"human\", \"vehicle\", \"construction\", \"background\", \"object\"]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"model_finetuned.pt\"  # path to your saved model\n",
    "IMAGE_PATH = \"path_to_image.jpg\"  # path to the image you want to infer\n",
    "IMAGE_SIZE = (512, 1024)  # resize your image as needed\n",
    "\n",
    "\n",
    "# === MODEL SETUP ===\n",
    "def load_model(model_path):\n",
    "    model = deeplabv3_resnet50(pretrained=False, num_classes=NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# === IMAGE PREPROCESSING ===\n",
    "def preprocess_image(image_path, image_size=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    transform_list = []\n",
    "    if image_size:\n",
    "        transform_list.append(transforms.Resize(image_size))\n",
    "\n",
    "    transform_list.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    preprocess = transforms.Compose(transform_list)\n",
    "    image_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    return image_tensor, image\n",
    "\n",
    "\n",
    "# === INFERENCE ===\n",
    "def infer(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.to(DEVICE)\n",
    "        output = model(image_tensor)['out']  # [1, 7, H, W]\n",
    "        probs = torch.sigmoid(output)        # sigmoid for multi-label\n",
    "        prediction = (probs > 0.5).float()   # threshold to binary mask\n",
    "    return prediction.squeeze(0).cpu()  # [7, H, W]\n",
    "\n",
    "\n",
    "# === VISUALIZATION ===\n",
    "def visualize_prediction(prediction_tensor, original_image, class_names):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        axes[i].imshow(prediction_tensor[i], cmap='gray')\n",
    "        axes[i].set_title(class_names[i])\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    axes[-1].imshow(original_image)\n",
    "    axes[-1].set_title(\"Original Image\")\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    assert os.path.exists(MODEL_PATH), f\"Model not found at {MODEL_PATH}\"\n",
    "    assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\"\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(MODEL_PATH)\n",
    "\n",
    "    print(\"Preprocessing image...\")\n",
    "    img_tensor, orig_img = preprocess_image(IMAGE_PATH, image_size=IMAGE_SIZE)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    prediction = infer(model, img_tensor)\n",
    "\n",
    "    print(\"Visualizing result...\")\n",
    "    visualize_prediction(prediction, orig_img, CLASS_NAMES)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa83ac1",
   "metadata": {},
   "source": [
    "## Gropued classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee268f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Macro class index mapping\n",
    "MACRO_CLASSES = {\n",
    "    \"road\": 0,\n",
    "    \"flat\": 1,\n",
    "    \"human\": 2,\n",
    "    \"vehicle\": 3,\n",
    "    \"construction\": 4,\n",
    "    \"background\": 5,\n",
    "    \"object\": 6,  # auxiliary objectness channel\n",
    "}\n",
    "\n",
    "# Map from original label ID to (macro class or None, is_object)   [None is only for the poles and traffic signs and lights]\n",
    "CLASS_MAPPING = {\n",
    "    7: (\"road\", False), # road\n",
    "    8: (\"flat\", False), # sidewalk\n",
    "    11: (\"construction\", False), # building\n",
    "    12: (\"construction\", False), # wall\n",
    "    13: (\"construction\", False), # fence\n",
    "    17: (None, True),  # pole\n",
    "    19: (None, True),  # traffic sign\n",
    "    20: (None, True),  # traffic light\n",
    "    21: (\"background\", False), # vegetation\n",
    "    22: (\"flat\", False), # terrain\n",
    "    23: (\"background\", False), # sky\n",
    "    24: (\"human\", True), # person\n",
    "    25: (\"human\", True), # rider\n",
    "    26: (\"vehicle\", True), # car\n",
    "    27: (\"vehicle\", True), # truck\n",
    "    28: (\"vehicle\", True), # bus\n",
    "    31: (\"vehicle\", True), # train\n",
    "    32: (\"vehicle\", True), # motorcycle\n",
    "    33: (\"vehicle\", True), # bicycle\n",
    "}\n",
    "\n",
    "# Prepare a mapping from original labels to macro class index (0 to 6)\n",
    "# For original labels mapped to None macro class (like poles, signs, lights), only 'object' class (6) will be set.\n",
    "\n",
    "LABEL_TO_MACRO_IDX = {}\n",
    "\n",
    "for original_id, (macro_class, is_object) in CLASS_MAPPING.items():\n",
    "    if macro_class is not None:\n",
    "        LABEL_TO_MACRO_IDX[original_id] = MACRO_CLASSES[macro_class]\n",
    "    else:\n",
    "        # For None macro class, we don't assign a macro_idx (only object channel will be set)\n",
    "        LABEL_TO_MACRO_IDX[original_id] = None\n",
    "\n",
    "\n",
    "def convert_label_to_multilabel_one_hot(label: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts label [H, W] with Cityscapes original IDs into a multi-label one-hot encoding tensor [7, H, W].\n",
    "    The last channel (index 6) corresponds to the 'object' auxiliary channel.\n",
    "    \"\"\"\n",
    "    height, width = label.shape\n",
    "    multilabel = torch.zeros((7, height, width), dtype=torch.float32)\n",
    "\n",
    "    for original_id, (_, is_object) in CLASS_MAPPING.items():\n",
    "        mask = (label == original_id)\n",
    "        macro_idx = LABEL_TO_MACRO_IDX[original_id]\n",
    "\n",
    "        if macro_idx is not None:\n",
    "            multilabel[macro_idx][mask] = 1.0\n",
    "\n",
    "        if is_object:\n",
    "            multilabel[MACRO_CLASSES[\"object\"]][mask] = 1.0\n",
    "\n",
    "    return multilabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c362fe0",
   "metadata": {},
   "source": [
    "### Visualization of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05221f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names_7 = [\"road\", \"flat\", \"human\", \"vehicle\", \"construction\", \"background\", \"object\"]\n",
    "\n",
    "def visualize_one_hot_vertical(one_hot, class_names=None, max_classes=7):\n",
    "    num_classes = min(one_hot.shape[0], max_classes)\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(5, 3 * num_classes))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(one_hot[i], cmap='gray')\n",
    "        title = f\"Class {i}\" if class_names is None else class_names[i]\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592d278",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ...  # your input tensor [H, W] with original Cityscapes label IDs\n",
    "\n",
    "multilabel = convert_label_to_multilabel_one_hot(label)  # shape [7, H, W]\n",
    "\n",
    "# Convert to numpy for visualization\n",
    "multilabel_np = multilabel.numpy()\n",
    "\n",
    "visualize_one_hot_vertical(multilabel_np, class_names_7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f30437",
   "metadata": {},
   "source": [
    "## BCEWithLogitsLoss (Boundary-aware Binary Cross Entropy Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f0e8f",
   "metadata": {},
   "source": [
    "The standard **BCEWithLogitsLoss** treats all pixels equally, which can be suboptimal when:\n",
    "\n",
    "-The object boundaries are thin and imbalanced in area.\n",
    "\n",
    "-You want to detect unknown objects that are often isolated or occluded, making boundaries key discriminators.\n",
    "\n",
    "A **Boundary-Aware BCE Loss** adds extra weight to boundary pixels, improving:\n",
    "\n",
    "-Edge delineation\n",
    "\n",
    "-Segmentation of small or novel objects\n",
    "\n",
    "-Generalization in open-set recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4aa29c",
   "metadata": {},
   "source": [
    "### Boundary-Aware Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "Let $P(x) = \\sigma(f(x))$ be the predicted probability at pixel $x$, where $f(x)$ is the raw logit output and $\\sigma(\\cdot)$ is the sigmoid activation. Let $Y(x) \\in \\{0, 1\\}$ be the ground truth label, and $w(x)$ be a boundary-based weight.\n",
    "\n",
    "The **Boundary-Aware BCE Loss** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}}^{\\text{boundary}} = -\\sum_{x} w(x) \\left[ Y(x) \\cdot \\log(P(x)) + (1 - Y(x)) \\cdot \\log(1 - P(x)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w(x) > 1$ for pixels near object boundaries,  \n",
    "- $w(x) = 1$ for all other pixels,  \n",
    "- and $P(x) = \\frac{1}{1 + e^{-f(x)}}$ is the sigmoid of the network output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bf504",
   "metadata": {},
   "source": [
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoundaryAwareBCELoss(nn.Module):\n",
    "    def __init__(self, edge_weight=5.0, dilation=3, use_sobel=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edge_weight (float): weight multiplier for boundary pixels.\n",
    "            dilation (int): how much to dilate boundary pixels.\n",
    "            use_sobel (bool): whether to compute boundaries using Sobel filter.\n",
    "        \"\"\"\n",
    "        super(BoundaryAwareBCELoss, self).__init__()\n",
    "        self.edge_weight = edge_weight\n",
    "        self.dilation = dilation\n",
    "        self.use_sobel = use_sobel\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (B, 1, H, W) or (B, C, H, W) raw outputs from the model\n",
    "            targets: (B, 1, H, W) or (B, C, H, W) ground truth binary masks\n",
    "        Returns:\n",
    "            Scalar boundary-aware BCE loss\n",
    "        \"\"\"\n",
    "        assert logits.shape == targets.shape, \"Shape mismatch between logits and targets\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weights = self.compute_boundary_weights(targets)\n",
    "\n",
    "        loss = self.bce(logits, targets)\n",
    "        weighted_loss = loss * weights\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "    def compute_boundary_weights(self, masks):\n",
    "        \"\"\"\n",
    "        Computes a per-pixel weight map based on boundary proximity.\n",
    "        Args:\n",
    "            masks: (B, C, H, W) binary ground truth masks\n",
    "        Returns:\n",
    "            weights: (B, C, H, W) tensor of pixel-wise weights\n",
    "        \"\"\"\n",
    "        if self.use_sobel:\n",
    "            edge = self.sobel_edges(masks)\n",
    "        else:\n",
    "            edge = self.laplacian_edges(masks)\n",
    "\n",
    "        # Optional dilation to expand boundary width\n",
    "        edge = F.max_pool2d(edge, kernel_size=self.dilation, stride=1, padding=self.dilation // 2)\n",
    "        weights = torch.ones_like(masks)\n",
    "        weights[edge > 0] = self.edge_weight\n",
    "        return weights\n",
    "\n",
    "    def sobel_edges(self, masks):\n",
    "        \"\"\"\n",
    "        Computes edges using Sobel operator.\n",
    "        \"\"\"\n",
    "        kernel_x = torch.tensor([[-1, 0, 1],\n",
    "                                 [-2, 0, 2],\n",
    "                                 [-1, 0, 1]], dtype=torch.float32, device=masks.device).view(1, 1, 3, 3)\n",
    "\n",
    "        kernel_y = torch.tensor([[-1, -2, -1],\n",
    "                                 [ 0,  0,  0],\n",
    "                                 [ 1,  2,  1]], dtype=torch.float32, device=masks.device).view(1, 1, 3, 3)\n",
    "\n",
    "        edge = torch.zeros_like(masks)\n",
    "        for c in range(masks.shape[1]):\n",
    "            gx = F.conv2d(masks[:, c:c+1], kernel_x, padding=1)\n",
    "            gy = F.conv2d(masks[:, c:c+1], kernel_y, padding=1)\n",
    "            edge[:, c:c+1] = (gx**2 + gy**2).sqrt()\n",
    "        return (edge > 0).float()\n",
    "\n",
    "    def laplacian_edges(self, masks):\n",
    "        \"\"\"\n",
    "        Alternative: computes edges using Laplacian operator.\n",
    "        \"\"\"\n",
    "        kernel = torch.tensor([[0, 1, 0],\n",
    "                               [1, -4, 1],\n",
    "                               [0, 1, 0]], dtype=torch.float32, device=masks.device).view(1, 1, 3, 3)\n",
    "        edge = torch.zeros_like(masks)\n",
    "        for c in range(masks.shape[1]):\n",
    "            e = F.conv2d(masks[:, c:c+1], kernel, padding=1).abs()\n",
    "            edge[:, c:c+1] = (e > 0).float()\n",
    "        return edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cb5e",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f54472",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BoundaryAwareBCELoss(edge_weight=5.0)\n",
    "\n",
    "# Simulated logits and targets\n",
    "logits = torch.randn((2, 1, 256, 256), requires_grad=True)  # raw outputs\n",
    "targets = torch.randint(0, 2, (2, 1, 256, 256)).float()      # binary masks\n",
    "\n",
    "loss = loss_fn(logits, targets)\n",
    "loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
