{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty-Aware Road Obstacle Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we propose an implementation of Road Obstacle Identification architecture, with the addition of a certain threshold of confidence to uncertainty, thanks to a state-of-the-art approach to loss function computations, the so called \"Boundary Aware Binary Cross Entropy\". These are the main implemented topics:\n",
    "- **Multilabel One-Hot Encoding**\n",
    "- **DeepLabV3+ --> ResNet50**\n",
    "- **Boundary Aware BCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "#import segmentation_models_pytorch as smp\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version PyTorch was built with: {torch.version.cuda}\")\n",
    "try:\n",
    "    print(f\"CUDA runtime version: {torch._C._cuda_getCompiledVersion()}\")\n",
    "except AttributeError:\n",
    "    print(\"CUDA is not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1996734):\n",
    "    random.seed(seed)                      # Python\n",
    "    np.random.seed(seed)                   # NumPy\n",
    "    torch.manual_seed(seed)                # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)           # PyTorch GPU\n",
    "\n",
    "set_seed(1996734)  # Call this at the top of your script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro class index mapping\n",
    "MACRO_CLASSES = {\n",
    "    \"road\": 0,\n",
    "    \"flat\": 1,\n",
    "    \"human\": 2,\n",
    "    \"vehicle\": 3,\n",
    "    \"construction\": 4,\n",
    "    \"background\": 5,\n",
    "    \"object\": 6,  # auxiliary objectness channel\n",
    "}\n",
    "\n",
    "# Map from original label ID to (macro class or None, is_object)   [None is only for the poles and traffic signs and lights]\n",
    "CLASS_MAPPING = {\n",
    "    7: (\"road\", False), # road\n",
    "    8: (\"flat\", False), # sidewalk\n",
    "    11: (\"construction\", False), # building\n",
    "    12: (\"construction\", False), # wall\n",
    "    13: (\"construction\", False), # fence\n",
    "    17: (None, True),  # pole\n",
    "    19: (None, True),  # traffic sign\n",
    "    20: (None, True),  # traffic light\n",
    "    21: (\"background\", False), # vegetation\n",
    "    22: (\"flat\", False), # terrain\n",
    "    23: (\"background\", False), # sky\n",
    "    24: (\"human\", True), # person\n",
    "    25: (\"human\", True), # rider\n",
    "    26: (\"vehicle\", True), # car\n",
    "    27: (\"vehicle\", True), # truck\n",
    "    28: (\"vehicle\", True), # bus\n",
    "    31: (\"vehicle\", True), # train\n",
    "    32: (\"vehicle\", True), # motorcycle\n",
    "    33: (\"vehicle\", True), # bicycle\n",
    "}\n",
    "\n",
    "# Set the relative path for the dataset\n",
    "relative_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing the Datasets**\n",
    "\n",
    "Here is the function to fix the structure of the two datasets, downloaded from the official CityScapes and LostAndFound websites, from their original form to a preferred one, following the structure defined therein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cityscapes(path_in, path_out, image_folder_in=\"leftImg8bit\", mask_folder_in=\"gtFine\", is_delete=False): # is_delete=True if you want to delete the city folders after copying\n",
    "    \"\"\"\n",
    "    Fixes the CityScapes dataset by renaming the files and removing the city folders.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "   \n",
    "    for split in splits:\n",
    "        count = 1\n",
    "        img_dir = os.path.join(path_in, image_folder_in, split)\n",
    "        mask_dir = os.path.join(path_in, mask_folder_in, split)\n",
    "\n",
    "        # New destination directories\n",
    "        img_out = os.path.join(path_out, 'img', split)\n",
    "        mask_out = os.path.join(path_out, 'mask', split)\n",
    "        os.makedirs(img_out, exist_ok=True)\n",
    "        os.makedirs(mask_out, exist_ok=True)\n",
    "\n",
    "        # Iterate on sub-folders\n",
    "        for city in os.listdir(img_dir):\n",
    "            city_img_dir = os.path.join(img_dir, city)\n",
    "            city_mask_dir = os.path.join(mask_dir, city)\n",
    "\n",
    "            if not os.path.isdir(city_img_dir):\n",
    "                continue  # Skips non-directory files\n",
    "\n",
    "            for filename in os.listdir(city_img_dir):\n",
    "                if not filename.endswith('leftImg8bit.png'):\n",
    "                    continue\n",
    "                img_path = os.path.join(city_img_dir, filename)\n",
    "                base_prefix = filename.replace('_leftImg8bit.png', '')\n",
    "\n",
    "                # Renames and copies RGB\n",
    "                new_base = f\"{split}{count}\"\n",
    "                ext = '.png'\n",
    "                new_img_name = f\"{new_base}{ext}\"\n",
    "                shutil.copy(img_path, os.path.join(img_out, new_img_name))\n",
    "\n",
    "                if split != 'test': # For test split, we only copy the image\n",
    "                    # Renames and copies all 'label' associated files\n",
    "                    suffixes = ['_gtFine_labelIds.png', '_gtFine_color.png', '_gtFine_instanceIds.png', '_gtFine_polygons.json']\n",
    "                    for suffix in suffixes:\n",
    "                        if suffix == '_gtFine_labelIds.png':\n",
    "                            original_name = base_prefix + suffix\n",
    "                            source = os.path.join(city_mask_dir, original_name)\n",
    "                            if os.path.exists(source):\n",
    "                                new_name = f\"{new_base}_m.png\"  # es. train1_m.png\n",
    "                                shutil.copy(source, os.path.join(mask_out, new_name))\n",
    "                        else:\n",
    "                            continue\n",
    "                count += 1\n",
    "                    \n",
    "            if is_delete:        \n",
    "                # Cleans city folders if empty\n",
    "                if os.path.isdir(city_img_dir) and not os.listdir(city_img_dir):\n",
    "                    os.rmdir(city_img_dir)\n",
    "                if os.path.isdir(city_mask_dir) and not os.listdir(city_mask_dir):\n",
    "                    os.rmdir(city_mask_dir)\n",
    "\n",
    "                # Removes split folders if empty\n",
    "                for d in [img_dir, mask_dir]:\n",
    "                    if os.path.isdir(d) and not os.listdir(d):\n",
    "                        os.rmdir(d)\n",
    "\n",
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing cityscapes dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_cityscapes(relative_path + 'cityscapes', relative_path + 'cityscapes_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lostandfound(path_in, path_out, image_folder_in=\"leftImg8bit\", mask_folder_in=\"gtCoarse\", is_delete=False): # is_delete=True if you want to delete the city folders after copying\n",
    "    \"\"\"\n",
    "    Fixes the LostAndFound dataset by renaming the files and removing the city folders.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'test']\n",
    "   \n",
    "    for split in splits:\n",
    "        count = 1\n",
    "        img_dir = os.path.join(path_in, image_folder_in, split)\n",
    "        mask_dir = os.path.join(path_in, mask_folder_in, split)\n",
    "\n",
    "        # New destination directories\n",
    "        img_out = os.path.join(path_out, 'img', split)\n",
    "        mask_out = os.path.join(path_out, 'mask', split)\n",
    "        os.makedirs(img_out, exist_ok=True)\n",
    "        os.makedirs(mask_out, exist_ok=True)\n",
    "\n",
    "        # Iterate on sub-folders\n",
    "        for city in os.listdir(img_dir):\n",
    "            city_img_dir = os.path.join(img_dir, city)\n",
    "            city_mask_dir = os.path.join(mask_dir, city)\n",
    "\n",
    "            if not os.path.isdir(city_img_dir):\n",
    "                continue  # Skips non-directory files\n",
    "\n",
    "            for filename in os.listdir(city_img_dir):\n",
    "                if not filename.endswith('leftImg8bit.png'):\n",
    "                    continue\n",
    "                img_path = os.path.join(city_img_dir, filename)\n",
    "                base_prefix = filename.replace('_leftImg8bit.png', '')\n",
    "\n",
    "                # Renames and copies RGB\n",
    "                new_base = f\"{split}{count}\"\n",
    "                ext = '.png'\n",
    "                new_img_name = f\"{new_base}{ext}\"\n",
    "                shutil.copy(img_path, os.path.join(img_out, new_img_name))\n",
    "\n",
    "                if split != 'test': # For test split, we only copy the image\n",
    "                    # Renames and copies all 'label' associated files\n",
    "                    suffixes = ['_gtCoarse_labelIds.png', '_gtCoarse_color.png', '_gtCoarse_instanceIds.png', '_gtCoarse_labelTrainIds.png', '_gtCoarse_polygons.json']\n",
    "                    for suffix in suffixes:\n",
    "                        if suffix == '_gtCoarse_labelIds.png':\n",
    "                            original_name = base_prefix + suffix\n",
    "                            source = os.path.join(city_mask_dir, original_name)\n",
    "                            if os.path.exists(source):\n",
    "                                new_name = f\"{new_base}_m.png\"  # es. train1_m.png\n",
    "                                shutil.copy(source, os.path.join(mask_out, new_name))\n",
    "                        else:\n",
    "                            continue\n",
    "                count += 1\n",
    "                    \n",
    "            if is_delete:        \n",
    "                # Cleans city folders if empty\n",
    "                if os.path.isdir(city_img_dir) and not os.listdir(city_img_dir):\n",
    "                    os.rmdir(city_img_dir)\n",
    "                if os.path.isdir(city_mask_dir) and not os.listdir(city_mask_dir):\n",
    "                    os.rmdir(city_mask_dir)\n",
    "\n",
    "                # Removes split folders if empty\n",
    "                for d in [img_dir, mask_dir]:\n",
    "                    if os.path.isdir(d) and not os.listdir(d):\n",
    "                        os.rmdir(d)\n",
    "\n",
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing lostandfound dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_lostandfound(relative_path + 'lostandfound', relative_path + 'lostandfound_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting Data**\n",
    "\n",
    "With these functions, we convert labels and images from the dataset into PyTorch tensors, in order to feed them to the network and begin the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a mapping from original labels to macro class index (0 to 6)\n",
    "# For original labels mapped to None macro class (like poles, signs, lights), only 'object' class (6) will be set.\n",
    "\n",
    "LABEL_TO_MACRO_IDX = {}\n",
    "\n",
    "for original_id, (macro_class, is_object) in CLASS_MAPPING.items():\n",
    "    if macro_class is not None:\n",
    "        LABEL_TO_MACRO_IDX[original_id] = MACRO_CLASSES[macro_class]\n",
    "    else:\n",
    "        # For None macro class, we don't assign a macro_idx (only object channel will be set)\n",
    "        LABEL_TO_MACRO_IDX[original_id] = None\n",
    "\n",
    "\n",
    "def convert_label_to_multilabel_one_hot(label):\n",
    "    \"\"\"\n",
    "    Converts 2D label mask [H, W] with Cityscapes original IDs into a multi-label one-hot encoding tensor [7, H, W].\n",
    "    The last channel (index 6) corresponds to the 'object' auxiliary channel.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the spatial dimensions of the label mask\n",
    "    height, width = label.shape\n",
    "\n",
    "    # Initialize the output tensor with 7 channels (macro-classes), filled with zeros.\n",
    "    # Each channel corresponds to a macro-class.\n",
    "    multilabel = torch.zeros((7, height, width), dtype=torch.float32)\n",
    "\n",
    "    # Iterate over each original class ID defined in the CLASS_MAPPING\n",
    "    for original_id, (_, is_object) in CLASS_MAPPING.items():\n",
    "\n",
    "        # Create a boolean mask of where the input label equals the current original class ID\n",
    "        mask = (label == original_id)\n",
    "\n",
    "        # Look up the macro-class index for this class ID, or None if it doesn't belong to any\n",
    "        macro_idx = LABEL_TO_MACRO_IDX[original_id]\n",
    "\n",
    "        # If this class maps to a macro-class, set 1 at those pixel locations in the corresponding channel\n",
    "        if macro_idx is not None:\n",
    "            multilabel[macro_idx][mask] = 1.0  # Set the macro-class channel to 1 where the mask is True\n",
    "\n",
    "        # If this class is considered an 'object', also set the 'object' channel (index 6) to 1\n",
    "        if is_object:\n",
    "            multilabel[MACRO_CLASSES[\"object\"]][mask] = 1.0  # Set the object class channel to 1\n",
    "\n",
    "    # Return the resulting multi-label one-hot tensor of shape [7, H, W]\n",
    "    return multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundaries Detection**\n",
    "\n",
    "Here we compute the boundary mask on the ground truth segmentation labels, to tell the model ‚Äúthis is a boundary region, pay more attention here‚Äù.\n",
    "\n",
    "To do that we use morphological gradient (difference between the dilation and the erosion of the image):\n",
    "- **Dilation**: expand the region of each class, boundary pixels move away from the center of the region (each pixel replaced with maximum value in its neighborhood)\n",
    "- **Erosion**: reduce the region of each class, boundary pixels move toward the center of the region (each pixel replaced with minimum value in its neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_mask(label_mask, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Computes the boundary mask from a label mask using morphological operations.\n",
    "    Args:\n",
    "        label_mask (numpy.ndarray): Input label mask with shape [H, W].\n",
    "        kernel_size (int): Size of the kernel for morphological operations.\n",
    "    Returns:\n",
    "        numpy.ndarray: Boundary mask with shape [H, W].\n",
    "    \"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilated = cv2.dilate(label_mask, kernel, iterations=2)\n",
    "    eroded = cv2.erode(label_mask, kernel, iterations=2)\n",
    "    boundary = (dilated != eroded).astype(np.uint8)\n",
    "    return boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process multiple masks in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_mask_batch(label_masks, kernel_size=3, iterations=2):\n",
    "    \"\"\"\n",
    "    Computes the boundary mask for a batch of label masks using morphological operations.\n",
    "    Args:\n",
    "        label_masks (torch.Tensor): Tensor of shape [B, H, W] or [B, 1, H, W], with binary masks (0 or 1).\n",
    "        kernel_size (int): Size of the kernel for morphological operations.\n",
    "        iterations (int): Number of times to apply dilation and erosion.\n",
    "    Returns:\n",
    "        torch.Tensor: Boundary masks of shape [B, H, W], dtype=torch.uint8.\n",
    "    \"\"\"\n",
    "\n",
    "    if label_masks.dim() == 3:\n",
    "        label_masks = label_masks.unsqueeze(1)  # [B, 1, H, W]\n",
    "\n",
    "    # Ensure float type for convolution\n",
    "    label_masks = label_masks.float()\n",
    "\n",
    "    # Define kernel (morphological structuring element)\n",
    "    device = label_masks.device\n",
    "    kernel = torch.ones((1, 1, kernel_size, kernel_size), device=device)\n",
    "\n",
    "    padding = kernel_size // 2\n",
    "\n",
    "    # Apply dilation\n",
    "    dilated = label_masks\n",
    "    for _ in range(iterations):\n",
    "        dilated = F.conv2d(dilated, kernel, padding=padding)\n",
    "        dilated = (dilated > 0).float()\n",
    "\n",
    "    # Apply erosion\n",
    "    eroded = label_masks\n",
    "    for _ in range(iterations):\n",
    "        eroded = F.conv2d(eroded, kernel, padding=padding)\n",
    "        eroded = (eroded == kernel.numel()).float()\n",
    "\n",
    "    # Compute boundary: difference between dilated and eroded regions\n",
    "    boundary = (dilated != eroded).float()\n",
    "\n",
    "    return boundary.squeeze(1).byte()  # Return shape [B, H, W], uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to show the aforementioned preprocessing, useful to visualize the data we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_7 = [\"road\", \"flat\", \"human\", \"vehicle\", \"construction\", \"background\", \"object\"]\n",
    "\n",
    "def visualize_one_hot_vertical(one_hot, class_names=None, max_classes=7):\n",
    "    \"\"\"\n",
    "    Visualizes the one-hot encoded masks vertically.\n",
    "    \"\"\"\n",
    "    num_classes = min(one_hot.shape[0], max_classes)\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(5, 3 * num_classes))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(one_hot[i], cmap='gray')\n",
    "        title = f\"Class {i}\" if class_names is None else class_names[i]\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_erosion_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the erosion mask of a label mask.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(label_mask, kernel, iterations=2)\n",
    "    plt.figure()\n",
    "    plt.imshow(erosion, cmap='gray')\n",
    "    plt.title(\"Erosion Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_dilation_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the dilation mask of a label mask.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilation = cv2.dilate(label_mask, kernel, iterations=2)\n",
    "    plt.figure()\n",
    "    plt.imshow(dilation, cmap='gray')\n",
    "    plt.title(\"Dilation Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_boundary_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the boundary mask.\n",
    "    \"\"\"\n",
    "    boundary = get_boundary_mask(label_mask)\n",
    "    plt.figure()\n",
    "    plt.imshow(boundary, cmap='gray')\n",
    "    plt.title(\"Boundary Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uos_heatmap(img_tensor, uos_tensor):\n",
    "    \"\"\"\n",
    "    Superpose the heatmap of the unknown objectness score to the image.\n",
    "    img_tensor: [3, H, W], torch.Tensor in [0, 1] or [0, 255]\n",
    "    uos_tensor: [H, W], torch.Tensor\n",
    "    \"\"\"\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    if img.max() <= 1.0:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    uos = uos_tensor.cpu().numpy()\n",
    "    uos = (uos - uos.min()) / (uos.max() - uos.min() + 1e-6)  # normalize\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(uos, cmap='hot', alpha=0.5)\n",
    "    plt.title(\"Unknown Objectness Score Heatmap for val_set\")\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for images\n",
    "##############\n",
    "\n",
    "resized_height = 512\n",
    "resized_width = 1024\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((resized_height, resized_width)),  # Resize to half the original size\n",
    "    T.ToTensor(),  # converts in [0, 1], shape [3, H, W]\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#############\n",
    "\n",
    "class CityscapesTrainEvalDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=transform):\n",
    "        self.transform = transform \n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "\n",
    "        # Collect all image paths\n",
    "        self.img_paths = list(self.img_dir.rglob(\"*.png\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "\n",
    "        # Derive corresponding mask path by adding '_m' before the extension\n",
    "        mask_name = img_path.stem + \"_m.png\"\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        if not mask_path.exists():\n",
    "            print(f\"Warning: No mask found for image: {img_path.name}\")\n",
    "\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "\n",
    "        # Load and preprocess mask\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale (single channel)\n",
    "\n",
    "        # Resize mask with nearest neighbor interpolation\n",
    "        resized_mask = mask.resize((resized_width, resized_height), resample=Image.NEAREST)\n",
    "\n",
    "        mask_np = np.array(resized_mask, dtype=np.uint8)\n",
    "\n",
    "        mask_tensor = torch.as_tensor(mask_np, dtype=torch.uint8)\n",
    "\n",
    "        mask_onehot = convert_label_to_multilabel_one_hot(mask_tensor)\n",
    "\n",
    "        return img, mask_onehot, mask_np\n",
    "    \n",
    "\n",
    "class CityscapesTestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=transform):\n",
    "        self.img_paths = sorted(Path(img_dir).rglob(\"*.png\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "\n",
    "        return img, str(img_path.name)  # Return the filename for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/train', relative_path + 'cityscapes_f/mask/train')\n",
    "val_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/val', relative_path + 'cityscapes_f/mask/val')\n",
    "test_set = CityscapesTestDataset(relative_path + 'cityscapes_f/img/test')\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last=True to ensure all batches have the same size\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "for imgs, masks, original_mask in val_loader:\n",
    "    print(\"Batch of images shape:\", imgs.shape)  # Should be [B, 3, H, W]\n",
    "    print(\"Batch of masks shape:\", masks.shape)  # Should be [B, 7, H, W]\n",
    "    print(\"Original mask shape:\", original_mask.shape)  # Should be [B, H, W]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_hot_vertical(masks[1], class_names=class_names_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDeepLabV3(nn.Module):\n",
    "    def __init__(self, n_classes=7):\n",
    "        super().__init__()\n",
    "        # Load pretrained model\n",
    "        self.model = deeplabv3_resnet50(pretrained=True)\n",
    "        \n",
    "        # Replace classifier to output 7 channels with sigmoid\n",
    "        self.model.classifier[-1] = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=n_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)['out']\n",
    "        return torch.sigmoid(x)  # Apply sigmoid for multilabel outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundary Aware BCE**\n",
    "\n",
    "Following the format of how to create a custom loss function according to:\n",
    "https://medium.com/we-talk-data/crafting-custom-loss-functions-in-pytorch-an-advanced-guide-830ff717163e\n",
    "\n",
    "- lambda_weight: weight lambda of the formula\n",
    "- pred: tensor of the outputs of the sigmoid head, the dimension is (B, C, H, W) with B batch size, C classes ...\n",
    "- target: ground truth one hot encoded with dimension (B, C, H, W)\n",
    "- boundary_mask: mask computed using get_boundary_mask() where 1 indicates boundary pixel, dimension (B, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryAwareBCELoss(nn.Module):\n",
    "    def __init__(self, lambda_weight=3.0):\n",
    "        super().__init__()\n",
    "        self.lambda_weight = lambda_weight\n",
    "\n",
    "    def forward(self, pred, target, boundary_mask):\n",
    "        #to avoid log(0)\n",
    "        eps = 1e-7\n",
    "\n",
    "        #standard BCE loss\n",
    "        bce = -(target * torch.log(pred + eps) + (1 - target) * torch.log(1 - pred + eps))\n",
    "        normal_term = bce.mean()\n",
    "\n",
    "        boundary_mask = boundary_mask.float()\n",
    "        #expansion to (B, C, H, W) to do the multiplication\n",
    "\n",
    "        if boundary_mask.dim() == 3:\n",
    "            boundary_mask = boundary_mask.unsqueeze(1)\n",
    "            \n",
    "        boundary_mask = boundary_mask.expand(-1, pred.shape[1], -1, -1)\n",
    "\n",
    "        #boundary aware BCE loss\n",
    "        boundary_bce = bce * boundary_mask\n",
    "        num_boundary_pixels = boundary_mask.sum(dim=(1, 2, 3)).clamp(min=1.0) #boundary pixels of each image\n",
    "        boundary_loss = boundary_bce.sum(dim=(1, 2, 3)) / num_boundary_pixels\n",
    "        boundary_term = boundary_loss.mean()\n",
    "\n",
    "        return normal_term + self.lambda_weight * boundary_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_erosion_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_dilation_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_boundary_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# Parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "current_iter = 0\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask).to(device)\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR update\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # ---- VALIDATION STEP ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask).to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "    \n",
    "    # ---- EARLY STOPPING LOGIC ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'weights/best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'current_iter': current_iter,\n",
    "}, 'weights/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume the training from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model and Loss ---\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "resume_epochs = 1  # how many more epochs to train\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# --- Checkpoint Info ---\n",
    "checkpoint_path = \"weights/checkpoint.pth\"\n",
    "start_epoch = 0\n",
    "current_iter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# --- Load Checkpoint if Exists ---\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    current_iter = checkpoint['current_iter']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resumed at epoch {start_epoch}, iter {current_iter}, best val loss {best_val_loss:.4f}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "total_epochs = start_epoch + resume_epochs\n",
    "max_iter = total_epochs * len(train_loader)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask).to(device)\n",
    "\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR scheduling\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask).to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"weights/best_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "    checkpoint_path = f\"weights/checkpoint_{epoch + 1}.pth\"\n",
    "    # --- Save Checkpoint ---\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'current_iter': current_iter,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "boundary_iteration_param = [3, 4, 5]  # arguments for get_boundary_mask_batch\n",
    "lambda_weights = [3.0, 5.0, 8.0]  # different lambda weights for BCE loss\n",
    "\n",
    "# Training parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 1\n",
    "patience = 3\n",
    "\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "\n",
    "for boundary_iteration in boundary_iteration_param:\n",
    "    for lambda_weight in lambda_weights:\n",
    "        print(f\"\\nüîß Starting training with get_boundary_mask_batch={boundary_iteration} and lambda={lambda_weight}\\n\")\n",
    "\n",
    "        # Model and loss\n",
    "        model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "        criterion = BoundaryAwareBCELoss(lambda_weight=lambda_weight)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        \n",
    "        max_iter = num_epochs * len(train_loader)\n",
    "        current_iter = 0\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        early_stop = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(imgs)\n",
    "                boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iteration).to(device)\n",
    "                loss = criterion(preds, masks, boundary_masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Poly LR schedule\n",
    "                current_iter += 1\n",
    "                lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "            # ---- VALIDATION ----\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for imgs, masks, original_mask in val_loader:\n",
    "                    imgs, masks = imgs.to(device), masks.to(device)\n",
    "                    preds = model(imgs)\n",
    "                    boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iteration).to(device)\n",
    "                    loss = criterion(preds, masks, boundary_masks)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "            # ---- EARLY STOPPING AND MODEL SAVING ----\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                counter = 0\n",
    "                # Save best model\n",
    "                filename = f\"weights/best_model_boundary_{boundary_iteration}_lambda_{lambda_weight}.pth\"\n",
    "                torch.save(model.state_dict(), filename)\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "                    early_stop = True\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß Starting training with get_boundary_mask_batch=3 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:22<00:00,  1.48it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3606 | Val Loss: 0.2576 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=3 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:10<00:00,  1.52it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5003 | Val Loss: 0.3691 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=3 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:13<00:00,  1.51it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7100 | Val Loss: 0.5326 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=4 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:47<00:00,  1.41it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3573 | Val Loss: 0.2527 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=4 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:48<00:00,  1.41it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5022 | Val Loss: 0.3632 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=4 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [08:43<00:00,  1.42it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7044 | Val Loss: 0.5229 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=5 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [09:27<00:00,  1.31it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3573 | Val Loss: 0.2531 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=5 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [09:40<00:00,  1.28it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5001 | Val Loss: 0.3661 | LR: 0.000000\n",
    "\n",
    "üîß Starting training with get_boundary_mask_batch=5 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [09:26<00:00,  1.31it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7162 | Val Loss: 0.5273 | LR: 0.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model architecture\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "model.to(device)  # move to GPU or CPU as appropriate\n",
    "\n",
    "checkpoint = torch.load('weights/checkpoint.pth', map_location=device)\n",
    "\n",
    "# Load the saved state_dicts\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1  # to continue training\n",
    "best_val_loss = checkpoint['loss']     # for early stopping, optional\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, masks, original_mask) in enumerate(tqdm(val_loader, desc=\"Validation\")):  # Assume val_loader is your validation DataLoader\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        \n",
    "        # Compute boundary masks for the ground truth masks\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask)  # Ensure this returns tensor on device\n",
    "\n",
    "        boundary_masks = boundary_masks.to(device)\n",
    "        \n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save each prediction tensor as .pt file\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_tensor = preds[i].cpu()  # Move to CPU\n",
    "            filename = os.path.join(\"saved_predictions/\", f\"image_{batch_idx * preds.shape[0] + 1 + i}.pt\")\n",
    "            torch.save(pred_tensor, filename)\n",
    "        \n",
    "        if batch_idx > 10:  # Print first 10 predictions for debugging\n",
    "            break\n",
    "\n",
    "avg_loss = total_loss / len(val_loader)\n",
    "print(f\"Validation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pixel Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measures the percentage of correctly predicted pixels.\n",
    "\n",
    "def pixel_accuracy(preds, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes pixel accuracy for multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "    Returns:\n",
    "        float: Pixel accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    correct = (preds == targets).float()\n",
    "    return correct.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Per-Class IoU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures overlap between predicted and true regions per class.\n",
    "\n",
    "def iou_per_class(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes IoU for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        list: IoU for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    ious = []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        union = (pred_cls + target_cls - pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        iou = (intersection + eps) / (union + eps)\n",
    "        ious.append(iou.mean().item())\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Mean IOUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of IoUs over all classes.\n",
    "\n",
    "def mean_iou(preds, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes mean IoU across all classes.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "    Returns:\n",
    "        float: Mean IoU across all classes.\n",
    "    \"\"\"\n",
    "    per_class_iou = iou_per_class(preds, targets, threshold)\n",
    "    return sum(per_class_iou) / len(per_class_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Dice Coefficient (F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also called the S√∏rensen-Dice index, especially used for imbalanced segmentation.\n",
    "\n",
    "def dice_score(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes the Dice score for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        list: Dice scores for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    scores = []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        union = pred_cls.sum(dim=(1, 2)) + target_cls.sum(dim=(1, 2))\n",
    "        dice = (2 * intersection + eps) / (union + eps)\n",
    "        scores.append(dice.mean().item())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Precision and Recall per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes precision and recall for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        tuple: (precisions, recalls) where each is a list of values for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    precisions, recalls = [], []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        tp = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        fp = (pred_cls * (1 - target_cls)).sum(dim=(1, 2))\n",
    "        fn = ((1 - pred_cls) * target_cls).sum(dim=(1, 2))\n",
    "        precision = (tp + eps) / (tp + fp + eps)\n",
    "        recall = (tp + eps) / (tp + fn + eps)\n",
    "        precisions.append(precision.mean().item())\n",
    "        recalls.append(recall.mean().item())\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Suggested Usage (after inference loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks, original_mask = next(iter(val_loader))  # Get a batch from the validation set\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "preds = (model(imgs))  # if output is logits\n",
    "preds = preds.detach().cpu()\n",
    "masks = masks.cpu()\n",
    "\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy(preds, masks))\n",
    "print(\"Mean IoU:\", mean_iou(preds, masks))\n",
    "print(\"Dice per class:\", dice_score(preds, masks))\n",
    "prec, rec = precision_recall(preds, masks)\n",
    "print(\"Precision per class:\", prec)\n",
    "print(\"Recall per class:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloured Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_INDEX_COLORS = [\n",
    "    (128, 64, 128),         # road - Purple\n",
    "    (244, 35, 232),         # flat - Pink\n",
    "    (220, 20, 60),          # human - Red\n",
    "    (0, 0, 142),            # vehicle - Blue\n",
    "    (70, 70, 70),           # construction - Gray\n",
    "    (107, 142, 35),         # background - Green\n",
    "]\n",
    "\n",
    "OUTPUT_DIR_COLOR = \"converted_predictions/color\"\n",
    "OUTPUT_DIR_OBJECT = \"converted_predictions/object\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_COLOR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_OBJECT, exist_ok=True)\n",
    "\n",
    "\n",
    "def convert_macroclass_to_color(pred_class):\n",
    "    \"\"\"Convert [H, W] prediction (0-5) to RGB image using macro class colors\"\"\"\n",
    "    pred_np = pred_class.cpu().numpy()\n",
    "    h, w = pred_np.shape\n",
    "    color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for class_idx, color in enumerate(CLASS_INDEX_COLORS):\n",
    "        mask = pred_np == class_idx\n",
    "        color_img[mask] = color\n",
    "        \n",
    "    return Image.fromarray(color_img)\n",
    "\n",
    "\n",
    "def save_object_mask_channel(pred_object):\n",
    "    \"\"\"Compute binary object mask image\"\"\"\n",
    "    object_mask = (pred_object > 0.5).cpu().numpy().astype(np.uint8) * 255\n",
    "    return Image.fromarray(object_mask)\n",
    "\n",
    "\n",
    "def process_prediction_file(pred_tensor: torch.Tensor, index: int):\n",
    "    \"\"\"\n",
    "    Saves the macro-class color image and binary object mask image of a single prediction tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_tensor: [7, H, W] tensor output from the model\n",
    "    - index: integer to use in the filename (e.g., 1 for image_1)\n",
    "    \"\"\"\n",
    "    name = f\"image_{index}\"\n",
    "\n",
    "    # Color\n",
    "    pred_macro = pred_tensor[:6]  # [6, H, W]\n",
    "    class_indices = pred_macro.argmax(dim=0)\n",
    "    color_img = convert_macroclass_to_color(class_indices)\n",
    "    color_img.save(os.path.join(OUTPUT_DIR_COLOR, f\"{name}_color.png\"))\n",
    "\n",
    "    # Object mask\n",
    "    object_img = save_object_mask_channel(pred_tensor[6])\n",
    "    object_img.save(os.path.join(OUTPUT_DIR_OBJECT, f\"{name}_object.png\"))\n",
    "\n",
    "\n",
    "INPUT_DIR = \"saved_predictions\"\n",
    "prediction_files = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith(\".pt\")])\n",
    "\n",
    "for pred_file in sorted(prediction_files):\n",
    "    idx = int(os.path.splitext(pred_file)[0].split('_')[1])\n",
    "    \n",
    "    path = os.path.join(INPUT_DIR, pred_file)\n",
    "    pred_tensor = torch.load(path)  # [7, H, W]\n",
    "    process_prediction_file(pred_tensor, idx)\n",
    "\n",
    "print(\"All predictions processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Uknown Objectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unknown_objectness_score(preds):\n",
    "    \"\"\"\n",
    "    Computes the unknown objectness score from the model predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Model predictions of shape [B, C, H, W] where C includes objectness channel.\n",
    "    Returns:\n",
    "        torch.Tensor: Unknown objectness scores of shape [B, H, W].\n",
    "    \"\"\"\n",
    "    obj_scores = preds[:, 6, :, :]\n",
    "    class_scores = preds[:, 0:6, :, :]\n",
    "    \n",
    "    unknown_scores = torch.prod(1 - class_scores, dim=1)\n",
    "    uos = obj_scores * unknown_scores\n",
    "    return uos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the heatmap superposed on the image\n",
    "idx = 3\n",
    "img, mask, orig_mask = val_set[idx]\n",
    "model.eval()\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "img = denormalize(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    uos = compute_unknown_objectness_score(model(img.unsqueeze(0).to(device)))[0]\n",
    "    # plt.imshow(uos.cpu().numpy(), cmap='hot')\n",
    "    # plt.title(f\"UOS for val_set[{idx}]\")\n",
    "    # plt.colorbar()\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "uos_heatmap(img, uos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
