{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty-Aware Road Obstacle Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we propose an implementation of Road Obstacle Identification architecture, with the addition of a certain threshold of confidence to uncertainty, thanks to a state-of-the-art approach to loss function computations, the so called \"Boundary Aware Binary Cross Entropy\". These are the main implemented topics:\n",
    "- **Multilabel One-Hot Encoding**\n",
    "- **DeepLabV3+ --> ResNet50**\n",
    "- **Boundary Aware BCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "#import segmentation_models_pytorch as smp\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version PyTorch was built with: {torch.version.cuda}\")\n",
    "try:\n",
    "    print(f\"CUDA runtime version: {torch._C._cuda_getCompiledVersion()}\")\n",
    "except AttributeError:\n",
    "    print(\"CUDA is not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1996734):\n",
    "    random.seed(seed)                      # Python\n",
    "    np.random.seed(seed)                   # NumPy\n",
    "    torch.manual_seed(seed)                # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)           # PyTorch GPU\n",
    "\n",
    "set_seed(1996734)  # Call this at the top of your script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro class index mapping\n",
    "MACRO_CLASSES = {\n",
    "    \"road\": 0,\n",
    "    \"flat\": 1,\n",
    "    \"human\": 2,\n",
    "    \"vehicle\": 3,\n",
    "    \"construction\": 4,\n",
    "    \"background\": 5,\n",
    "    \"object\": 6,  # auxiliary objectness channel\n",
    "}\n",
    "\n",
    "# Map from original label ID to (macro class or None, is_object)   [None is only for the poles and traffic signs and lights]\n",
    "CLASS_MAPPING = {\n",
    "    7: (\"road\", False), # road\n",
    "    8: (\"flat\", False), # sidewalk\n",
    "    11: (\"construction\", False), # building\n",
    "    12: (\"construction\", False), # wall\n",
    "    13: (\"construction\", False), # fence\n",
    "    17: (\"construction\", True),  # pole\n",
    "    19: (\"construction\", True),  # traffic sign\n",
    "    20: (\"construction\", True),  # traffic light\n",
    "    21: (\"background\", False), # vegetation\n",
    "    22: (\"flat\", False), # terrain\n",
    "    23: (\"background\", False), # sky\n",
    "    24: (\"human\", True), # person\n",
    "    25: (\"human\", True), # rider\n",
    "    26: (\"vehicle\", True), # car\n",
    "    27: (\"vehicle\", True), # truck\n",
    "    28: (\"vehicle\", True), # bus\n",
    "    31: (\"vehicle\", True), # train\n",
    "    32: (\"vehicle\", True), # motorcycle\n",
    "    33: (\"vehicle\", True), # bicycle\n",
    "}\n",
    "\n",
    "# Set the relative path for the dataset\n",
    "relative_path = '../../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing the Datasets**\n",
    "\n",
    "Here is the function to fix the structure of the two datasets, downloaded from the official CityScapes and LostAndFound websites, from their original form to a preferred one, following the structure defined therein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cityscapes(path_in, path_out, image_folder_in=\"leftImg8bit\", mask_folder_in=\"gtFine\", is_delete=False): # is_delete=True if you want to delete the city folders after copying\n",
    "    \"\"\"\n",
    "    Fixes the CityScapes dataset by renaming the files and removing the city folders.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'val', 'test']\n",
    "   \n",
    "    for split in splits:\n",
    "        count = 1\n",
    "        img_dir = os.path.join(path_in, image_folder_in, split)\n",
    "        mask_dir = os.path.join(path_in, mask_folder_in, split)\n",
    "\n",
    "        # New destination directories\n",
    "        img_out = os.path.join(path_out, 'img', split)\n",
    "        mask_out = os.path.join(path_out, 'mask', split)\n",
    "        os.makedirs(img_out, exist_ok=True)\n",
    "        os.makedirs(mask_out, exist_ok=True)\n",
    "\n",
    "        # Iterate on sub-folders\n",
    "        for city in os.listdir(img_dir):\n",
    "            city_img_dir = os.path.join(img_dir, city)\n",
    "            city_mask_dir = os.path.join(mask_dir, city)\n",
    "\n",
    "            if not os.path.isdir(city_img_dir):\n",
    "                continue  # Skips non-directory files\n",
    "\n",
    "            for filename in os.listdir(city_img_dir):\n",
    "                if not filename.endswith('leftImg8bit.png'):\n",
    "                    continue\n",
    "                img_path = os.path.join(city_img_dir, filename)\n",
    "                base_prefix = filename.replace('_leftImg8bit.png', '')\n",
    "\n",
    "                # Renames and copies RGB\n",
    "                new_base = f\"{split}{count}\"\n",
    "                ext = '.png'\n",
    "                new_img_name = f\"{new_base}{ext}\"\n",
    "                shutil.copy(img_path, os.path.join(img_out, new_img_name))\n",
    "\n",
    "                if split != 'test': # For test split, we only copy the image\n",
    "                    # Renames and copies all 'label' associated files\n",
    "                    suffixes = ['_gtFine_labelIds.png', '_gtFine_color.png', '_gtFine_instanceIds.png', '_gtFine_polygons.json']\n",
    "                    for suffix in suffixes:\n",
    "                        if suffix == '_gtFine_labelIds.png':\n",
    "                            original_name = base_prefix + suffix\n",
    "                            source = os.path.join(city_mask_dir, original_name)\n",
    "                            if os.path.exists(source):\n",
    "                                new_name = f\"{new_base}_m.png\"  # es. train1_m.png\n",
    "                                shutil.copy(source, os.path.join(mask_out, new_name))\n",
    "                        else:\n",
    "                            continue\n",
    "                count += 1\n",
    "                    \n",
    "            if is_delete:        \n",
    "                # Cleans city folders if empty\n",
    "                if os.path.isdir(city_img_dir) and not os.listdir(city_img_dir):\n",
    "                    os.rmdir(city_img_dir)\n",
    "                if os.path.isdir(city_mask_dir) and not os.listdir(city_mask_dir):\n",
    "                    os.rmdir(city_mask_dir)\n",
    "\n",
    "                # Removes split folders if empty\n",
    "                for d in [img_dir, mask_dir]:\n",
    "                    if os.path.isdir(d) and not os.listdir(d):\n",
    "                        os.rmdir(d)\n",
    "\n",
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing cityscapes dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_cityscapes(relative_path + 'cityscapes', relative_path + 'cityscapes_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lostandfound(path_in, path_out, image_folder_in=\"leftImg8bit\", mask_folder_in=\"gtCoarse\", is_delete=False): # is_delete=True if you want to delete the city folders after copying\n",
    "    \"\"\"\n",
    "    Fixes the LostAndFound dataset by renaming the files and removing the city folders.\n",
    "    \"\"\"\n",
    "    splits = ['train', 'test']\n",
    "   \n",
    "    for split in splits:\n",
    "        count = 1\n",
    "        img_dir = os.path.join(path_in, image_folder_in, split)\n",
    "        mask_dir = os.path.join(path_in, mask_folder_in, split)\n",
    "\n",
    "        # New destination directories\n",
    "        img_out = os.path.join(path_out, 'img', split)\n",
    "        mask_out = os.path.join(path_out, 'mask', split)\n",
    "        os.makedirs(img_out, exist_ok=True)\n",
    "        os.makedirs(mask_out, exist_ok=True)\n",
    "\n",
    "        # Iterate on sub-folders\n",
    "        for city in os.listdir(img_dir):\n",
    "            city_img_dir = os.path.join(img_dir, city)\n",
    "            city_mask_dir = os.path.join(mask_dir, city)\n",
    "\n",
    "            if not os.path.isdir(city_img_dir):\n",
    "                continue  # Skips non-directory files\n",
    "\n",
    "            for filename in os.listdir(city_img_dir):\n",
    "                if not filename.endswith('leftImg8bit.png'):\n",
    "                    continue\n",
    "                img_path = os.path.join(city_img_dir, filename)\n",
    "                base_prefix = filename.replace('_leftImg8bit.png', '')\n",
    "\n",
    "                # Renames and copies RGB\n",
    "                new_base = f\"{split}{count}\"\n",
    "                ext = '.png'\n",
    "                new_img_name = f\"{new_base}{ext}\"\n",
    "                shutil.copy(img_path, os.path.join(img_out, new_img_name))\n",
    "\n",
    "                if split != 'test': # For test split, we only copy the image\n",
    "                    # Renames and copies all 'label' associated files\n",
    "                    suffixes = ['_gtCoarse_labelIds.png', '_gtCoarse_color.png', '_gtCoarse_instanceIds.png', '_gtCoarse_labelTrainIds.png', '_gtCoarse_polygons.json']\n",
    "                    for suffix in suffixes:\n",
    "                        if suffix == '_gtCoarse_labelIds.png':\n",
    "                            original_name = base_prefix + suffix\n",
    "                            source = os.path.join(city_mask_dir, original_name)\n",
    "                            if os.path.exists(source):\n",
    "                                new_name = f\"{new_base}_m.png\"  # es. train1_m.png\n",
    "                                shutil.copy(source, os.path.join(mask_out, new_name))\n",
    "                        else:\n",
    "                            continue\n",
    "                count += 1\n",
    "                    \n",
    "            if is_delete:        \n",
    "                # Cleans city folders if empty\n",
    "                if os.path.isdir(city_img_dir) and not os.listdir(city_img_dir):\n",
    "                    os.rmdir(city_img_dir)\n",
    "                if os.path.isdir(city_mask_dir) and not os.listdir(city_mask_dir):\n",
    "                    os.rmdir(city_mask_dir)\n",
    "\n",
    "                # Removes split folders if empty\n",
    "                for d in [img_dir, mask_dir]:\n",
    "                    if os.path.isdir(d) and not os.listdir(d):\n",
    "                        os.rmdir(d)\n",
    "\n",
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing lostandfound dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_lostandfound(relative_path + 'lostandfound', relative_path + 'lostandfound_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting Data**\n",
    "\n",
    "With these functions, we convert labels and images from the dataset into PyTorch tensors, in order to feed them to the network and begin the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a mapping from original labels to macro class index (0 to 6)\n",
    "# For original labels mapped to None macro class (like poles, signs, lights), only 'object' class (6) will be set.\n",
    "\n",
    "LABEL_TO_MACRO_IDX = {}\n",
    "\n",
    "for original_id, (macro_class, is_object) in CLASS_MAPPING.items():\n",
    "    if macro_class is not None:\n",
    "        LABEL_TO_MACRO_IDX[original_id] = MACRO_CLASSES[macro_class]\n",
    "    else:\n",
    "        # For None macro class, we don't assign a macro_idx (only object channel will be set)\n",
    "        LABEL_TO_MACRO_IDX[original_id] = None\n",
    "\n",
    "\n",
    "def convert_label_to_multilabel_one_hot(label, dataset):\n",
    "    \"\"\"\n",
    "    Converts 2D label mask [H, W] with Cityscapes original IDs into a multi-label one-hot encoding tensor [7, H, W].\n",
    "    The last channel (index 6) corresponds to the 'object' auxiliary channel.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the spatial dimensions of the label mask\n",
    "    height, width = label.shape\n",
    "\n",
    "    # Initialize the output tensor with 7 channels (macro-classes), filled with zeros.\n",
    "    # Each channel corresponds to a macro-class.\n",
    "    multilabel = torch.zeros((7, height, width), dtype=torch.float32)\n",
    "        \n",
    "    if dataset == \"cityscapes\":\n",
    "        # Iterate over each original class ID defined in the CLASS_MAPPING\n",
    "        for original_id, (_, is_object) in CLASS_MAPPING.items():\n",
    "\n",
    "            # Create a boolean mask of where the input label equals the current original class ID\n",
    "            mask = (label == original_id)\n",
    "\n",
    "            # Look up the macro-class index for this class ID, or None if it doesn't belong to any\n",
    "            macro_idx = LABEL_TO_MACRO_IDX[original_id]\n",
    "\n",
    "            # If this class maps to a macro-class, set 1 at those pixel locations in the corresponding channel\n",
    "            if macro_idx is not None:\n",
    "                multilabel[macro_idx][mask] = 1.0  # Set the macro-class channel to 1 where the mask is True\n",
    "\n",
    "            # If this class is considered an 'object', also set the 'object' channel (index 6) to 1\n",
    "            if is_object:\n",
    "                multilabel[MACRO_CLASSES[\"object\"]][mask] = 1.0  # Set the object class channel to 1\n",
    "    elif dataset == \"lostandfound\":\n",
    "        height, width = label.shape\n",
    "        multilabel = torch.zeros((7, height, width), dtype=torch.float32)\n",
    "\n",
    "        road_mask = (label == 1)\n",
    "        object_mask_1 = (label != 1)\n",
    "        object_mask_2 = (label != 0)\n",
    "        object_mask = object_mask_1 == object_mask_2\n",
    "\n",
    "        multilabel[MACRO_CLASSES[\"road\"]][road_mask] = 1.0\n",
    "        multilabel[MACRO_CLASSES[\"object\"]][object_mask] = 1.0\n",
    "    else:\n",
    "        print(\"you have to choose a dataset between cityscapes and lostandfound\\n \")\n",
    "\n",
    "    # Return the resulting multi-label one-hot tensor of shape [7, H, W]\n",
    "    return multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundaries Detection**\n",
    "\n",
    "Here we compute the boundary mask on the ground truth segmentation labels, to tell the model “this is a boundary region, pay more attention here”.\n",
    "\n",
    "To do that we use morphological gradient (difference between the dilation and the erosion of the image):\n",
    "- **Dilation**: expand the region of each class, boundary pixels move away from the center of the region (each pixel replaced with maximum value in its neighborhood)\n",
    "- **Erosion**: reduce the region of each class, boundary pixels move toward the center of the region (each pixel replaced with minimum value in its neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_mask(label_mask, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Computes the boundary mask from a label mask using morphological operations.\n",
    "    Args:\n",
    "        label_mask (numpy.ndarray): Input label mask with shape [H, W].\n",
    "        kernel_size (int): Size of the kernel for morphological operations.\n",
    "    Returns:\n",
    "        numpy.ndarray: Boundary mask with shape [H, W].\n",
    "    \"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilated = cv2.dilate(label_mask, kernel, iterations=2)\n",
    "    eroded = cv2.erode(label_mask, kernel, iterations=2)\n",
    "    boundary = (dilated != eroded).astype(np.uint8)\n",
    "    return boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process multiple masks in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_mask_batch(label_masks, kernel_size=3, iterations=2):\n",
    "    \"\"\"\n",
    "    Computes the boundary mask for a batch of label masks using morphological operations.\n",
    "    Args:\n",
    "        label_masks (torch.Tensor): Tensor of shape [B, H, W] or [B, 1, H, W], with binary masks (0 or 1).\n",
    "        kernel_size (int): Size of the kernel for morphological operations.\n",
    "        iterations (int): Number of times to apply dilation and erosion.\n",
    "    Returns:\n",
    "        torch.Tensor: Boundary masks of shape [B, H, W], dtype=torch.uint8.\n",
    "    \"\"\"\n",
    "\n",
    "    if label_masks.dim() == 3:\n",
    "        label_masks = label_masks.unsqueeze(1)  # [B, 1, H, W]\n",
    "\n",
    "    # Ensure float type for convolution\n",
    "    label_masks = label_masks.float()\n",
    "\n",
    "    # Define kernel (morphological structuring element)\n",
    "    device = label_masks.device\n",
    "    kernel = torch.ones((1, 1, kernel_size, kernel_size), device=device)\n",
    "\n",
    "    padding = kernel_size // 2\n",
    "\n",
    "    # Apply dilation\n",
    "    dilated = label_masks\n",
    "    for _ in range(iterations):\n",
    "        dilated = F.conv2d(dilated, kernel, padding=padding)\n",
    "        dilated = (dilated > 0).float()\n",
    "\n",
    "    # Apply erosion\n",
    "    eroded = label_masks\n",
    "    for _ in range(iterations):\n",
    "        eroded = F.conv2d(eroded, kernel, padding=padding)\n",
    "        eroded = (eroded == kernel.numel()).float()\n",
    "\n",
    "    # Compute boundary: difference between dilated and eroded regions\n",
    "    boundary = (dilated != eroded).float()\n",
    "\n",
    "    return boundary.squeeze(1).byte()  # Return shape [B, H, W], uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to show the aforementioned preprocessing, useful to visualize the data we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_7 = [\"road\", \"flat\", \"human\", \"vehicle\", \"construction\", \"background\", \"object\"]\n",
    "\n",
    "def visualize_one_hot_vertical(one_hot, class_names=None, max_classes=7):\n",
    "    \"\"\"\n",
    "    Visualizes the one-hot encoded masks vertically.\n",
    "    \"\"\"\n",
    "    num_classes = min(one_hot.shape[0], max_classes)\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(5, 3 * num_classes))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(one_hot[i], cmap='gray')\n",
    "        title = f\"Class {i}\" if class_names is None else class_names[i]\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_erosion_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the erosion mask of a label mask.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(label_mask, kernel, iterations=2)\n",
    "    plt.figure()\n",
    "    plt.imshow(erosion, cmap='gray')\n",
    "    plt.title(\"Erosion Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_dilation_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the dilation mask of a label mask.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilation = cv2.dilate(label_mask, kernel, iterations=2)\n",
    "    plt.figure()\n",
    "    plt.imshow(dilation, cmap='gray')\n",
    "    plt.title(\"Dilation Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_boundary_mask(label_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the boundary mask.\n",
    "    \"\"\"\n",
    "    boundary = get_boundary_mask(label_mask)\n",
    "    plt.figure()\n",
    "    plt.imshow(boundary, cmap='gray')\n",
    "    plt.title(\"Boundary Mask\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uos_heatmap(img_tensor, uos_tensor):\n",
    "    \"\"\"\n",
    "    Superpose the heatmap of the unknown objectness score to the image.\n",
    "    img_tensor: [3, H, W], torch.Tensor in [0, 1] or [0, 255]\n",
    "    uos_tensor: [H, W], torch.Tensor\n",
    "    \"\"\"\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    if img.max() <= 1.0:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    uos = uos_tensor.cpu().numpy()\n",
    "    uos = (uos - uos.min()) / (uos.max() - uos.min() + 1e-6)  # normalize\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(uos, cmap='hot', alpha=0.5)\n",
    "    plt.title(\"Unknown Objectness Score Heatmap for val_set\")\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for images\n",
    "##############\n",
    "\n",
    "resized_height = 512\n",
    "resized_width = 1024\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((resized_height, resized_width)),  # Resize to half the original size\n",
    "    T.ToTensor(),  # converts in [0, 1], shape [3, H, W]\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#############\n",
    "\n",
    "class CityscapesTrainEvalDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=transform):\n",
    "        self.transform = transform \n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "\n",
    "        # Collect all image paths\n",
    "        self.img_paths = list(self.img_dir.rglob(\"*.png\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "\n",
    "        # Derive corresponding mask path by adding '_m' before the extension\n",
    "        mask_name = img_path.stem + \"_m.png\"\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        if not mask_path.exists():\n",
    "            print(f\"Warning: No mask found for image: {img_path.name}\")\n",
    "\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "\n",
    "        # Load and preprocess mask\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale (single channel)\n",
    "\n",
    "        # Resize mask with nearest neighbor interpolation\n",
    "        resized_mask = mask.resize((resized_width, resized_height), resample=Image.NEAREST)\n",
    "\n",
    "        mask_np = np.array(resized_mask, dtype=np.uint8)\n",
    "\n",
    "        mask_tensor = torch.as_tensor(mask_np, dtype=torch.uint8)\n",
    "\n",
    "        mask_onehot = convert_label_to_multilabel_one_hot(mask_tensor, \"cityscapes\")\n",
    "\n",
    "        return img, mask_onehot, mask_np\n",
    "    \n",
    "\n",
    "class CityscapesTestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=transform):\n",
    "        self.img_paths = sorted(Path(img_dir).rglob(\"*.png\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "\n",
    "        return img, str(img_path.name)  # Return the filename for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([4, 3, 512, 1024])\n",
      "Batch of masks shape: torch.Size([4, 7, 512, 1024])\n",
      "Original mask shape: torch.Size([4, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "train_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/train', relative_path + 'cityscapes_f/mask/train')\n",
    "val_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/val', relative_path + 'cityscapes_f/mask/val')\n",
    "test_set = CityscapesTestDataset(relative_path + 'cityscapes_f/img/test')\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last=True to ensure all batches have the same size\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "for imgs, masks, original_mask in val_loader:\n",
    "    print(\"Batch of images shape:\", imgs.shape)  # Should be [B, 3, H, W]\n",
    "    print(\"Batch of masks shape:\", masks.shape)  # Should be [B, 7, H, W]\n",
    "    print(\"Original mask shape:\", original_mask.shape)  # Should be [B, H, W]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_hot_vertical(masks[1], class_names=class_names_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDeepLabV3(nn.Module):\n",
    "    def __init__(self, n_classes=7):\n",
    "        super().__init__()\n",
    "        # Load pretrained model\n",
    "        self.model = deeplabv3_resnet50(pretrained=True)\n",
    "        \n",
    "        # Replace classifier to output 7 channels with sigmoid\n",
    "        self.model.classifier[-1] = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=n_classes,\n",
    "            kernel_size=1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)['out']\n",
    "        return torch.sigmoid(x)  # Apply sigmoid for multilabel outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundary Aware BCE**\n",
    "\n",
    "Following the format of how to create a custom loss function according to:\n",
    "https://medium.com/we-talk-data/crafting-custom-loss-functions-in-pytorch-an-advanced-guide-830ff717163e\n",
    "\n",
    "- lambda_weight: weight lambda of the formula\n",
    "- pred: tensor of the outputs of the sigmoid head, the dimension is (B, C, H, W) with B batch size, C classes ...\n",
    "- target: ground truth one hot encoded with dimension (B, C, H, W)\n",
    "- boundary_mask: mask computed using get_boundary_mask() where 1 indicates boundary pixel, dimension (B, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryAwareBCELoss(nn.Module):\n",
    "    def __init__(self, lambda_weight=3.0):\n",
    "        super().__init__()\n",
    "        self.lambda_weight = lambda_weight\n",
    "\n",
    "    def forward(self, pred, target, boundary_mask):\n",
    "        #to avoid log(0)\n",
    "        eps = 1e-7\n",
    "\n",
    "        #standard BCE loss\n",
    "        bce = -(target * torch.log(pred + eps) + (1 - target) * torch.log(1 - pred + eps))\n",
    "        normal_term = bce.mean()\n",
    "\n",
    "        boundary_mask = boundary_mask.float()\n",
    "        #expansion to (B, C, H, W) to do the multiplication\n",
    "\n",
    "        if boundary_mask.dim() == 3:\n",
    "            boundary_mask = boundary_mask.unsqueeze(1)\n",
    "            \n",
    "        boundary_mask = boundary_mask.expand(-1, pred.shape[1], -1, -1)\n",
    "\n",
    "        #boundary aware BCE loss\n",
    "        boundary_bce = bce * boundary_mask\n",
    "        num_boundary_pixels = boundary_mask.sum(dim=(1, 2, 3)).clamp(min=1.0) #boundary pixels of each image\n",
    "        boundary_loss = boundary_bce.sum(dim=(1, 2, 3)) / num_boundary_pixels\n",
    "        boundary_term = boundary_loss.mean()\n",
    "\n",
    "        return normal_term + self.lambda_weight * boundary_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_erosion_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_dilation_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_boundary_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda 3, iterations 7 up to 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First 5 epochs training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miketango2002/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/miketango2002/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1: 100%|██████████| 743/743 [07:57<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | Train Loss: 0.3558 | Val Loss: 0.2559 | LR: 0.008181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 743/743 [07:48<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] | Train Loss: 0.2210 | Val Loss: 0.2378 | LR: 0.006314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 743/743 [07:34<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] | Train Loss: 0.1928 | Val Loss: 0.2190 | LR: 0.004384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 743/743 [07:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] | Train Loss: 0.1749 | Val Loss: 0.2138 | LR: 0.002349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 743/743 [07:38<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] | Train Loss: 0.1645 | Val Loss: 0.2113 | LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# Parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 5\n",
    "boundary_iterations = 7\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "current_iter = 0\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR update\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # ---- VALIDATION STEP ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache() \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "    \n",
    "    # ---- EARLY STOPPING LOGIC ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'current_iter': current_iter,\n",
    "        }, 'weights/new_model_boundary_7_lambda_3.0_epoch_5.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From epoch 5 up to epoch 10 by resuming a checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from weights/new_model_boundary_7_lambda_3.0_epoch_5.pth\n",
      "Resumed at epoch 5, iter 3715, best val loss 0.2113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 743/743 [07:26<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Train Loss: 0.1676 | Val Loss: 0.2239 | LR: 0.004384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 743/743 [07:26<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] | Train Loss: 0.1565 | Val Loss: 0.2063 | LR: 0.003384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 743/743 [07:31<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] | Train Loss: 0.1481 | Val Loss: 0.2072 | LR: 0.002349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 743/743 [07:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] | Train Loss: 0.1428 | Val Loss: 0.2060 | LR: 0.001259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 743/743 [07:39<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] | Train Loss: 0.1396 | Val Loss: 0.2055 | LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Loss ---\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "resume_epochs = 5  # how many more epochs to train\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# --- Checkpoint Info ---\n",
    "weights_name = \"new_model_boundary_7_lambda_3.0_epoch_5.pth\" # Name of the weights file, e.g., \"model_weights.pth\"\n",
    "checkpoint_path = \"weights/\" + weights_name\n",
    "output_path = \"weights/\" + \"new_model_boundary_7_lambda_3.0_epoch_10.pth\" # It changes during training\n",
    "start_epoch = 0\n",
    "current_iter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "boundary_iterations = 7  # Number of iterations for boundary mask computation\n",
    "\n",
    "# --- Load Checkpoint if Exists ---\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    current_iter = checkpoint['current_iter']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resumed at epoch {start_epoch}, iter {current_iter}, best val loss {best_val_loss:.4f}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "total_epochs = start_epoch + resume_epochs\n",
    "max_iter = total_epochs * len(train_loader)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR scheduling\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'current_iter': current_iter,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, output_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda 3, iterations 5 up to 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First 5 epochs training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 743/743 [07:37<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | Train Loss: 0.3580 | Val Loss: 0.2597 | LR: 0.008181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 743/743 [07:28<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] | Train Loss: 0.2206 | Val Loss: 0.2385 | LR: 0.006314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 743/743 [07:37<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] | Train Loss: 0.1928 | Val Loss: 0.2188 | LR: 0.004384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 743/743 [07:36<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] | Train Loss: 0.1749 | Val Loss: 0.2125 | LR: 0.002349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 743/743 [07:32<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] | Train Loss: 0.1646 | Val Loss: 0.2104 | LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# Parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 5\n",
    "boundary_iterations = 5\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "current_iter = 0\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR update\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # ---- VALIDATION STEP ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "    \n",
    "    # ---- EARLY STOPPING LOGIC ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'current_iter': current_iter,\n",
    "        }, 'weights/new_model_boundary_5_lambda_3.0_epoch_5.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From epoch 5 up to epoch 10 by resuming a checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from weights/new_model_boundary_5_lambda_3.0_epoch_5.pth\n",
      "Resumed at epoch 5, iter 3715, best val loss 0.2104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 743/743 [07:28<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Train Loss: 0.1649 | Val Loss: 0.2173 | LR: 0.004384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 743/743 [07:27<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] | Train Loss: 0.1556 | Val Loss: 0.2167 | LR: 0.003384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 743/743 [07:27<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] | Train Loss: 0.1484 | Val Loss: 0.2123 | LR: 0.002349\n",
      "Early stopping at epoch 8. Best val loss: 0.2104\n"
     ]
    }
   ],
   "source": [
    "# --- Model and Loss ---\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "resume_epochs = 5  # how many more epochs to train\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# --- Checkpoint Info ---\n",
    "weights_name = \"new_model_boundary_5_lambda_3.0_epoch_5.pth\" # Name of the weights file, e.g., \"model_weights.pth\"\n",
    "checkpoint_path = \"weights/\" + weights_name\n",
    "output_path = \"weights/\" + \"new_model_boundary_5_lambda_3.0_epoch_10.pth\" # It changes during training\n",
    "start_epoch = 0\n",
    "current_iter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "boundary_iterations = 5  # Number of iterations for boundary mask computation\n",
    "\n",
    "# --- Load Checkpoint if Exists ---\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    current_iter = checkpoint['current_iter']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Resumed at epoch {start_epoch}, iter {current_iter}, best val loss {best_val_loss:.4f}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "total_epochs = start_epoch + resume_epochs\n",
    "max_iter = total_epochs * len(train_loader)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR scheduling\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}] | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'current_iter': current_iter,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, output_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "boundary_iteration_param = [3, 5, 7]  # arguments for get_boundary_mask_batch\n",
    "lambda_weights = [3.0]  # different lambda weights for BCE loss\n",
    "\n",
    "# Training parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 1\n",
    "patience = 3\n",
    "\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "\n",
    "for boundary_iteration in boundary_iteration_param:\n",
    "    for lambda_weight in lambda_weights:\n",
    "        print(f\"\\n🔧 Starting training with get_boundary_mask_batch={boundary_iteration} and lambda={lambda_weight}\\n\")\n",
    "\n",
    "        # Model and loss\n",
    "        model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "        criterion = BoundaryAwareBCELoss(lambda_weight=lambda_weight)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        \n",
    "        max_iter = num_epochs * len(train_loader)\n",
    "        current_iter = 0\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        early_stop = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(imgs)\n",
    "                boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iteration).to(device)\n",
    "                loss = criterion(preds, masks, boundary_masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Poly LR schedule\n",
    "                current_iter += 1\n",
    "                lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "            # ---- VALIDATION ----\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for imgs, masks, original_mask in val_loader:\n",
    "                    imgs, masks = imgs.to(device), masks.to(device)\n",
    "                    preds = model(imgs)\n",
    "                    boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iteration).to(device)\n",
    "                    loss = criterion(preds, masks, boundary_masks)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "            # ---- EARLY STOPPING AND MODEL SAVING ----\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                counter = 0\n",
    "                # Save best model\n",
    "                filename = f\"weights/model_boundary_{boundary_iteration}_lambda_{lambda_weight}.pth\"\n",
    "                torch.save(model.state_dict(), filename)\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"⏹️ Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "                    early_stop = True\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔧 Starting training with get_boundary_mask_batch=3 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:22<00:00,  1.48it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3606 | Val Loss: 0.2576 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=3 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:10<00:00,  1.52it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5003 | Val Loss: 0.3691 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=3 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:13<00:00,  1.51it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7100 | Val Loss: 0.5326 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=4 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:47<00:00,  1.41it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3573 | Val Loss: 0.2527 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=4 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:48<00:00,  1.41it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5022 | Val Loss: 0.3632 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=4 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [08:43<00:00,  1.42it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7044 | Val Loss: 0.5229 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=5 and lambda=3.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [09:27<00:00,  1.31it/s]\n",
    "Epoch [1/1] | Train Loss: 0.3573 | Val Loss: 0.2531 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=5 and lambda=5.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [09:40<00:00,  1.28it/s]\n",
    "Epoch [1/1] | Train Loss: 0.5001 | Val Loss: 0.3661 | LR: 0.000000\n",
    "\n",
    "🔧 Starting training with get_boundary_mask_batch=5 and lambda=8.0\n",
    "\n",
    "Epoch 1: 100%|██████████| 743/743 [09:26<00:00,  1.31it/s]\n",
    "Epoch [1/1] | Train Loss: 0.7162 | Val Loss: 0.5273 | LR: 0.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model architecture\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "model.to(device)  # move to GPU or CPU as appropriate\n",
    "\n",
    "weight_name = \"new_model_boundary_5_lambda_3.0.pth\"  # Change this to the desired model name\n",
    "\n",
    "checkpoint = torch.load('weights/' + weight_name, map_location=device)\n",
    "\n",
    "# Load the saved state_dicts\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1  # to continue training\n",
    "best_val_loss = checkpoint['loss']     # for early stopping, optional\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, masks, original_mask) in enumerate(tqdm(val_loader, desc=\"Validation\")):  # Assume val_loader is your validation DataLoader\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        \n",
    "        # Compute boundary masks for the ground truth masks\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask)  # Ensure this returns tensor on device\n",
    "\n",
    "        boundary_masks = boundary_masks.to(device)\n",
    "        \n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save each prediction tensor as .pt file\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_tensor = preds[i].cpu()  # Move to CPU\n",
    "            filename = os.path.join(\"saved_predictions/\", f\"image_{batch_idx * preds.shape[0] + 1 + i}.pt\")\n",
    "            torch.save(pred_tensor, filename)\n",
    "        \n",
    "        if batch_idx > 10:  # Print first 10 predictions for debugging\n",
    "            break\n",
    "\n",
    "avg_loss = total_loss / len(val_loader)\n",
    "print(f\"Validation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pixel Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measures the percentage of correctly predicted pixels.\n",
    "\n",
    "def pixel_accuracy(preds, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes pixel accuracy for multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "    Returns:\n",
    "        float: Pixel accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    correct = (preds == targets).float()\n",
    "    return correct.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Per-Class IoU (Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures overlap between predicted and true regions per class.\n",
    "\n",
    "def iou_per_class(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes IoU for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        list: IoU for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    ious = []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        union = (pred_cls + target_cls - pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        iou = (intersection + eps) / (union + eps)\n",
    "        ious.append(iou.mean().item())\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Mean IOUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of IoUs over all classes.\n",
    "\n",
    "def mean_iou(preds, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes mean IoU across all classes.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "    Returns:\n",
    "        float: Mean IoU across all classes.\n",
    "    \"\"\"\n",
    "    per_class_iou = iou_per_class(preds, targets, threshold)\n",
    "    return sum(per_class_iou) / len(per_class_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Dice Coefficient (F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also called the Sørensen-Dice index, especially used for imbalanced segmentation.\n",
    "\n",
    "def dice_score(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes the Dice score for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        list: Dice scores for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    scores = []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        union = pred_cls.sum(dim=(1, 2)) + target_cls.sum(dim=(1, 2))\n",
    "        dice = (2 * intersection + eps) / (union + eps)\n",
    "        scores.append(dice.mean().item())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Precision and Recall per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(preds, targets, threshold=0.5, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Computes precision and recall for each class in multi-label predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted probabilities of shape [B, C, H, W].\n",
    "        targets (torch.Tensor): Ground truth labels of shape [B, C, H, W].\n",
    "        threshold (float): Threshold for converting probabilities to binary predictions.\n",
    "        eps (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        tuple: (precisions, recalls) where each is a list of values for each class.\n",
    "    \"\"\"\n",
    "    preds = (preds > threshold).float()\n",
    "    precisions, recalls = [], []\n",
    "    for cls in range(preds.shape[1]):\n",
    "        pred_cls = preds[:, cls]\n",
    "        target_cls = targets[:, cls]\n",
    "        tp = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        fp = (pred_cls * (1 - target_cls)).sum(dim=(1, 2))\n",
    "        fn = ((1 - pred_cls) * target_cls).sum(dim=(1, 2))\n",
    "        precision = (tp + eps) / (tp + fp + eps)\n",
    "        recall = (tp + eps) / (tp + fn + eps)\n",
    "        precisions.append(precision.mean().item())\n",
    "        recalls.append(recall.mean().item())\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Suggested Usage (after inference loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks, original_mask = next(iter(val_loader))  # Get a batch from the validation set\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "preds = (model(imgs))  # if output is logits\n",
    "preds = preds.detach().cpu()\n",
    "masks = masks.cpu()\n",
    "\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy(preds, masks))\n",
    "print(\"Mean IoU:\", mean_iou(preds, masks))\n",
    "print(\"Dice per class:\", dice_score(preds, masks))\n",
    "prec, rec = precision_recall(preds, masks)\n",
    "print(\"Precision per class:\", prec)\n",
    "print(\"Recall per class:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coloured Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_INDEX_COLORS = [\n",
    "    (128, 64, 128),         # road - Purple\n",
    "    (244, 35, 232),         # flat - Pink\n",
    "    (220, 20, 60),          # human - Red\n",
    "    (0, 0, 142),            # vehicle - Blue\n",
    "    (70, 70, 70),           # construction - Gray\n",
    "    (107, 142, 35),         # background - Green\n",
    "]\n",
    "\n",
    "OUTPUT_DIR_COLOR = \"converted_predictions/color\"\n",
    "OUTPUT_DIR_OBJECT = \"converted_predictions/object\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_COLOR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_OBJECT, exist_ok=True)\n",
    "\n",
    "\n",
    "def convert_macroclass_to_color(pred_class):\n",
    "    \"\"\"Convert [H, W] prediction (0-5) to RGB image using macro class colors\"\"\"\n",
    "    pred_np = pred_class.cpu().numpy()\n",
    "    h, w = pred_np.shape\n",
    "    color_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for class_idx, color in enumerate(CLASS_INDEX_COLORS):\n",
    "        mask = pred_np == class_idx\n",
    "        color_img[mask] = color\n",
    "        \n",
    "    return Image.fromarray(color_img)\n",
    "\n",
    "\n",
    "def save_object_mask_channel(pred_object):\n",
    "    \"\"\"Compute binary object mask image\"\"\"\n",
    "    object_mask = (pred_object > 0.7).cpu().numpy().astype(np.uint8) * 255\n",
    "    return Image.fromarray(object_mask)\n",
    "\n",
    "\n",
    "def process_prediction_file(pred_tensor: torch.Tensor, index: int):\n",
    "    \"\"\"\n",
    "    Saves the macro-class color image and binary object mask image of a single prediction tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_tensor: [7, H, W] tensor output from the model\n",
    "    - index: integer to use in the filename (e.g., 1 for image_1)\n",
    "    \"\"\"\n",
    "    name = f\"image_{index}\"\n",
    "\n",
    "    # Color\n",
    "    pred_macro = pred_tensor[:6]  # [6, H, W]\n",
    "    class_indices = pred_macro.argmax(dim=0)\n",
    "    color_img = convert_macroclass_to_color(class_indices)\n",
    "    color_img.save(os.path.join(OUTPUT_DIR_COLOR, f\"{name}_color.png\"))\n",
    "\n",
    "    # Object mask\n",
    "    object_img = save_object_mask_channel(pred_tensor[6])\n",
    "    object_img.save(os.path.join(OUTPUT_DIR_OBJECT, f\"{name}_object.png\"))\n",
    "\n",
    "\n",
    "INPUT_DIR = \"saved_predictions\"\n",
    "prediction_files = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith(\".pt\")])\n",
    "\n",
    "for pred_file in sorted(prediction_files):\n",
    "    idx = int(os.path.splitext(pred_file)[0].split('_')[1])\n",
    "    \n",
    "    path = os.path.join(INPUT_DIR, pred_file)\n",
    "    pred_tensor = torch.load(path)  # [7, H, W]\n",
    "    process_prediction_file(pred_tensor, idx)\n",
    "\n",
    "print(\"All predictions processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **UNKNOWN OBJECTNESS SCORE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Formal Definition**\n",
    "Let:\n",
    "\n",
    "- $\\sigma_c(x)$ be the sigmoid output (i.e., probability) for class $c$ at pixel $x$, for  \n",
    "  $c \\in \\{ \\text{road}, \\text{flat}, \\text{human}, \\text{vehicle}, \\text{construction}, \\text{background}, \\text{object} \\}$\n",
    "\n",
    "- $C = \\{ \\text{road}, \\text{flat}, \\text{human}, \\text{vehicle}, \\text{construction}, \\text{background} \\}$\n",
    "\n",
    "- $\\sigma_{\\text{obj}}(x)$ be the sigmoid output for the **object** class.\n",
    "\n",
    "---\n",
    "\n",
    "The **unknown objectness score** is defined as:\n",
    "\n",
    "$$\n",
    "S_{\\text{objectness}}(x) = \\sigma_{\\text{obj}}(x) \\cdot \\prod_{c \\in C} \\left(1 - \\sigma_c(x)\\right)\n",
    "$$\n",
    "\n",
    "This is a multiplicative uncertainty-aware object score, with the following interpretation:\n",
    "\n",
    "- High when the pixel is **likely to be an object**, i.e., $\\sigma_{\\text{obj}}(x) \\approx 1$, and  \n",
    "- The pixel is **unlikely to belong to any known class** $c \\in C$, i.e., $\\sigma_c(x) \\approx 0 \\ \\forall c$\n",
    "\n",
    "---\n",
    "\n",
    "In short, this score quantifies the **confidence that a pixel belongs to an object not previously known** in training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unknown_objectness_score(preds):\n",
    "    \"\"\"\n",
    "    Computes the unknown objectness score from the model predictions.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Model predictions of shape [B, C, H, W] where C includes objectness channel.\n",
    "    Returns:\n",
    "        torch.Tensor: Unknown objectness scores of shape [B, H, W].\n",
    "    \"\"\"\n",
    "    obj_scores = preds[:, 6, :, :]\n",
    "    class_scores = preds[:, 0:6, :, :]\n",
    "    \n",
    "    unknown_scores = torch.prod(1 - class_scores, dim=1)\n",
    "    uos = obj_scores * unknown_scores\n",
    "    return uos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the heatmap superposed on the image\n",
    "idx = 100\n",
    "img, mask, orig_mask = val_set[idx]\n",
    "\n",
    "# Load grayscale image (1 channel)\n",
    "image_path = f\"../../lostandfound_f/img/train/train{idx-1}.png\"\n",
    "img_lost_and_found = Image.open(image_path)  # 'L' mode = 8-bit pixels, black and white\n",
    "\n",
    "resized_height = 512\n",
    "resized_width = 1024\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((resized_height, resized_width)),  # Resize to half the original size\n",
    "    T.ToTensor(),  # converts in [0, 1], shape [3, H, W]\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img_lost_and_found = transform(img_lost_and_found)  # Apply the same transformation\n",
    "\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = MultiLabelDeepLabV3(n_classes=7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "model.to(device)  # move to GPU or CPU as appropriate\n",
    "\n",
    "weight_name = \"new_model_boundary_5_lambda_3.0.pth\"  # Change this to the desired model name\n",
    "\n",
    "checkpoint = torch.load('weights/' + weight_name, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "#img = denormalize(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    uos = compute_unknown_objectness_score(model(img_lost_and_found.unsqueeze(0).to(device)))[0]\n",
    "    # plt.imshow(uos.cpu().numpy(), cmap='hot')\n",
    "    # plt.title(f\"UOS for val_set[{idx}]\")\n",
    "    # plt.colorbar()\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "img_lost_and_found = denormalize(img_lost_and_found)\n",
    "uos_heatmap(img_lost_and_found, uos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONFORMAL PREDICTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Feasibility of Using Conformal Prediction with Sigmoid Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Conformal Prediction (CP) is frequently illustrated with classification models employing softmax outputs, as softmax naturally provides normalized class confidence scores. However, the theoretical foundation of CP does not depend on softmax per se—it only requires a well-calibrated **nonconformity score** that quantifies how atypical or uncertain a prediction is.\n",
    "\n",
    "In our case:\n",
    "\n",
    "* We replaced the softmax layer with **per-class sigmoid activations**, which is a valid approach for **multi-label semantic segmentation**, where a pixel may simultaneously belong to multiple classes (e.g., both `\"object\"` and `\"vehicle\"`).\n",
    "\n",
    "* We introduced a **total objectness score**, a measure combining the likelihood of a pixel belonging to the `\"object\"` class with its unlikeliness of belonging to any known (non-object) class. This score captures how strongly a pixel appears to be part of an obstacle—either known or unknown.\n",
    "\n",
    "* This total objectness score serves as a principled and expressive **nonconformity function**, particularly suitable for open-set scenarios. **Low objectness** implies conformity with **background** or known non-obstacle classes.\n",
    "**High objectness** indicates the pixel likely belongs to an **obstacle**.\n",
    "\n",
    "Hence, using Conformal Prediction in this sigmoid-based framework is not only feasible but especially advantageous for **uncertainty quantification in road obstacle identification**, including both known and unknown objects in the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Nonconformity Score: What is the Nonconformity Function For?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conformal prediction, the **nonconformity function α(x)** quantifies how **strange or unusual** a new sample x is **with respect to a calibration set**.\n",
    "\n",
    "-A **high** nonconformity score means the sample is **less similar** to the training/calibration data -> more \"uncertain\".\n",
    "\n",
    "-A **low** score means it behaves like samples seen before -> more \"conforming\".\n",
    "\n",
    "It is used to rank samples and to derive a threshold during calibration (based on a desired error rate ϵ) so that, at test time, you can decide whether to accept or reject a prediction, or to quantify uncertainty (e.g., assign confidence levels to predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Nonconformity Score: How to Define It in This Case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the **unknown objectness score**:\n",
    "\n",
    "$$\n",
    "S_{\\text{unk-objectness}}(x) = \\sigma_{\\text{obj}}(x) \\cdot \\prod_{c \\in C} \\left(1 - \\sigma_c(x)\\right)\n",
    "$$\n",
    "\n",
    "We define the **total obstacle objectness score** as:\n",
    "$$\n",
    "S_{\\text{obstacle}}(x) = \\sigma_{\\text{human}}(x) + \\sigma_{\\text{vehicle}}(x) + S_{\\text{unk-objectness}}(x)\n",
    "$$\n",
    "\n",
    "This score quantifies the confidence that a pixel belongs to any road obstacle — either known (human or vehicle) or unknown.\n",
    "\n",
    "<br>\n",
    "\n",
    "The **nonconformity score** for conformal prediction is then defined as:\n",
    "$$\n",
    "\\alpha(x) = 1 - S_{\\text{obstacle}}(x)\n",
    "$$\n",
    "\n",
    " ~In which higher values mean more \"strangeness\" (i.e., more non-conforming)~\n",
    "\n",
    "\n",
    "This transformation flips the interpretation appropriately:  \n",
    "a pixel with **low objectness score** gets a **high nonconformity score**, and vice versa.\n",
    "\n",
    "---\n",
    "\n",
    "This $\\alpha(x)$ is then suitable for:\n",
    "\n",
    "- Calibrate a threshold $q_{1-\\varepsilon}$ on a validation set as the $(1-\\varepsilon)$-quantile of the distribution of $\\alpha(x)$.\n",
    "\n",
    "- At test time, for each pixel $x$:\n",
    "$$\n",
    "\\text{If } \\alpha(x) \\leq q_{1-\\varepsilon}, \\text{ classify pixel } x \\text{ as belonging to a road obstacle (known or unknown).}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LOSTANDFOUND TRIALS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LostAndFoundTrainEvalDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=transform):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.img_paths = sorted(self.img_dir.rglob(\"*.png\"))[:1000] # Limit to 1000 images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_name = img_path.stem + \"_m.png\"\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        if not mask_path.exists():\n",
    "            print(f\"Warning: No mask found for image: {img_path.name}\")\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "\n",
    "        # Load and resize mask\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # grayscale\n",
    "        resized_mask = mask.resize((resized_width, resized_height), resample=Image.NEAREST)\n",
    "        mask_np = np.array(resized_mask, dtype=np.uint8)\n",
    "        mask_tensor = torch.as_tensor(mask_np, dtype=torch.uint8)\n",
    "\n",
    "        mask_onehot = convert_label_to_multilabel_one_hot(mask_tensor, \"lostandfound\")\n",
    "\n",
    "        return img, mask_onehot, mask_np  # [3,H,W], [7,H,W], [H,W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([4, 3, 512, 1024])\n",
      "Batch of masks shape: torch.Size([4, 7, 512, 1024])\n",
      "Original mask shape: torch.Size([4, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "train_set_lostandfound = LostAndFoundTrainEvalDataset(relative_path + 'lostandfound_f/img/train', relative_path + 'lostandfound_f/mask/train')\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader_lostandfound = DataLoader(train_set_lostandfound, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "for imgs, masks, original_mask in train_loader_lostandfound:\n",
    "    print(\"Batch of images shape:\", imgs.shape)  # Should be [B, 3, H, W]\n",
    "    print(\"Batch of masks shape:\", masks.shape)  # Should be [B, 7, H, W]\n",
    "    print(\"Original mask shape:\", original_mask.shape)  # Should be [B, H, W]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask, orig_mask = train_set[370]\n",
    "\n",
    "visualize_one_hot_vertical(mask, class_names=class_names_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Reverse the normalization transform\n",
    "unnormalize = T.Normalize(\n",
    "    mean=[-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "    std=[1/s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "# Unnormalize the image\n",
    "img_unnorm = unnormalize(img)\n",
    "img_np = img_unnorm.permute(1, 2, 0).numpy()\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_np)\n",
    "plt.title(f\"Oringinal Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning of the model using the LostAndFoundDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda 3, iterations 7, epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 11:   4%|▎         | 9/250 [00:07<03:13,  1.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m model.train()\n\u001b[32m     46\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFine-tune Epoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mLostAndFoundTrainEvalDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: No mask found for image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     22\u001b[39m     img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/PIL/Image.py:982\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    980\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universita/magistrale/anno_I/semestre_II/cv/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:389\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    388\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have imported these properly\n",
    "# from your_model_module import MultiLabelDeepLabV3, BoundaryAwareBCELoss, get_boundary_mask_batch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiLabelDeepLabV3(n_classes=7).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('weights/new_model_boundary_7_lambda_3.0_epoch_10.pth', map_location=device)\n",
    "\n",
    "# Load weights into model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Reinitialize optimizer\n",
    "fine_tune_lr = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "optimizer = optim.SGD(model.parameters(), lr=fine_tune_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Optionally load previous optimizer state\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Resume metadata\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "boundary_iterations = 7  # Number of iterations for boundary mask computation\n",
    "\n",
    "# Criterion\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "num_finetune_epochs = 5\n",
    "power = 0.9  # for poly LR schedule\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "current_iter = 0\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(start_epoch, start_epoch + num_finetune_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, masks, original_mask in tqdm(train_loader_lostandfound, desc=f\"Fine-tune Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly learning rate update\n",
    "        current_iter += 1\n",
    "        lr = fine_tune_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"[Fine-tune Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'current_iter': current_iter,\n",
    "        }, 'weights/fine_tuned_model_boundary_7_lambda_3.0_epoch_10.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
