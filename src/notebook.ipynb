{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "#import segmentation_models_pytorch as smp\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils import visualize_uos_with_conformal, nonconformity_score, fix_lostandfound, fix_cityscapes, CityscapesTrainEvalDataset, CityscapesTestDataset, visualize_one_hot_vertical, visualize_erosion_mask, visualize_dilation_mask, visualize_boundary_mask, LostAndFoundTrainEvalDataset, MultiLabelDeepLabV3, BoundaryAwareBCELoss, get_boundary_mask_batch, BoundaryAwareBCELossFineTuning, pixel_accuracy, mean_iou, dice_score, precision_recall, unknown_objectness_score, uos_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro class index mapping\n",
    "MACRO_CLASSES = {\n",
    "    \"road\": 0,\n",
    "    \"flat\": 1,\n",
    "    \"human\": 2,\n",
    "    \"vehicle\": 3,\n",
    "    \"construction\": 4,\n",
    "    \"background\": 5,\n",
    "    \"pole\": 6,\n",
    "    \"object\": 7,  # auxiliary objectness channel\n",
    "}\n",
    "\n",
    "# Map from original label ID to (macro class or None, is_object)   [None is only for the poles and traffic signs and lights]\n",
    "CLASS_MAPPING = {\n",
    "    7: (\"road\", False), # road\n",
    "    8: (\"flat\", False), # sidewalk\n",
    "    11: (\"construction\", False), # building\n",
    "    12: (\"construction\", False), # wall\n",
    "    13: (\"construction\", False), # fence\n",
    "    17: (\"pole\", True),  # pole\n",
    "    19: (\"pole\", True),  # traffic sign\n",
    "    20: (\"pole\", True),  # traffic light\n",
    "    21: (\"background\", False), # vegetation\n",
    "    22: (\"flat\", False), # terrain\n",
    "    23: (\"background\", False), # sky\n",
    "    24: (\"human\", True), # person\n",
    "    25: (\"human\", True), # rider\n",
    "    26: (\"vehicle\", True), # car\n",
    "    27: (\"vehicle\", True), # truck\n",
    "    28: (\"vehicle\", True), # bus\n",
    "    31: (\"vehicle\", True), # train\n",
    "    32: (\"vehicle\", True), # motorcycle\n",
    "    33: (\"vehicle\", True), # bicycle\n",
    "}\n",
    "\n",
    "class_names_8 = [\"road\", \"flat\", \"human\", \"vehicle\", \"construction\", \"background\", \"pole\", \"object\"]\n",
    "\n",
    "# Set the relative path for the dataset\n",
    "relative_path = '../../'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version PyTorch was built with: {torch.version.cuda}\")\n",
    "try:\n",
    "    print(f\"CUDA runtime version: {torch._C._cuda_getCompiledVersion()}\")\n",
    "except AttributeError:\n",
    "    print(\"CUDA is not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1996734):\n",
    "    random.seed(seed)                      # Python\n",
    "    np.random.seed(seed)                   # NumPy\n",
    "    torch.manual_seed(seed)                # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)           # PyTorch GPU\n",
    "\n",
    "set_seed(1996734)  # Call this at the top of your script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixing the Datasets**\n",
    "\n",
    "Here is the function to fix the structure of the two datasets, downloaded from the official CityScapes and LostAndFound websites, from their original form to a preferred one, following the structure defined therein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing cityscapes dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_cityscapes(relative_path + 'cityscapes', relative_path + 'cityscapes_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fixed = True\n",
    "if not is_fixed:\n",
    "    print(\"Fixing lostandfound dataset...\")\n",
    "    # Fix the dataset\n",
    "    fix_lostandfound(relative_path + 'datasets/lostandfound', relative_path + 'lostandfound_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/train', relative_path + 'cityscapes_f/mask/train')\n",
    "benchmark_set = CityscapesTrainEvalDataset(relative_path + 'cityscapes_f/img/val', relative_path + 'cityscapes_f/mask/val')\n",
    "\n",
    "# Split the training set into training, validation and calibration sets\n",
    "train_size = len(train_set)\n",
    "cal_and_val_size = int(0.2 * train_size)\n",
    "cal_size = int(0.5 * cal_and_val_size)  # 10% of the original training set size\n",
    "train_set, calib_and_val_set = random_split(train_set, [train_size - cal_and_val_size, cal_and_val_size])\n",
    "cal_set, val_set = random_split(calib_and_val_set, [cal_size, cal_and_val_size - cal_size])\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last=True to ensure all batches have the same size\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "cal_loader = DataLoader(cal_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "benchmark_loader = DataLoader(benchmark_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "for imgs, masks, original_mask in val_loader:\n",
    "    print(\"Batch of images shape:\", imgs.shape)  # Should be [B, 3, H, W]\n",
    "    print(\"Batch of masks shape:\", masks.shape)  # Should be [B, 8, H, W]\n",
    "    print(\"Original mask shape:\", original_mask.shape)  # Should be [B, H, W]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lostandfound = LostAndFoundTrainEvalDataset(relative_path + 'lostandfound_f/img/train', relative_path + 'lostandfound_f/mask/train')\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n",
    "train_size = int(train_ratio * len(dataset_lostandfound))\n",
    "val_size = len(dataset_lostandfound) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, val_subset = random_split(dataset_lostandfound, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_lostandfound = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader_lostandfound = DataLoader(val_subset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "for imgs, masks, original_mask in train_loader_lostandfound:\n",
    "    print(\"Batch of images shape:\", imgs.shape)  # Should be [B, 3, H, W]\n",
    "    print(\"Batch of masks shape:\", masks.shape)  # Should be [B, 8, H, W]\n",
    "    print(\"Original mask shape:\", original_mask.shape)  # Should be [B, H, W]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_hot_vertical(masks[1], class_names=class_names_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_erosion_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_dilation_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')))\n",
    "visualize_boundary_mask(np.array(Image.open(relative_path + 'cityscapes_f/mask/train/train1_m.png')), iterations=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelDeepLabV3(n_classes=8).to(device)\n",
    "\n",
    "criterion = BoundaryAwareBCELoss(lambda_weight=3.0)\n",
    "\n",
    "# Parameters\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "power = 0.9\n",
    "num_epochs = 2\n",
    "boundary_iterations = 7\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_epochs * len(train_loader)\n",
    "current_iter = 0\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for imgs, masks, original_mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly LR update\n",
    "        current_iter += 1\n",
    "        lr = initial_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # ---- VALIDATION STEP ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache() \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "    \n",
    "    # ---- EARLY STOPPING LOGIC ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'current_iter': current_iter,\n",
    "        }, 'weights/new_weights/model_boundary_7_epoch_2.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have imported these properly\n",
    "# from your_model_module import MultiLabelDeepLabV3, BoundaryAwareBCELoss, get_boundary_mask_batch\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiLabelDeepLabV3(n_classes=8).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('weights/new_weights/model_boundary_5_lambda_3.0_epochs_2.pth', map_location=device)\n",
    "\n",
    "# Load weights into model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Reinitialize optimizer\n",
    "fine_tune_lr = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "optimizer = optim.SGD(model.parameters(), lr=fine_tune_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Optionally load previous optimizer state\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Resume metadata\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_val_loss = float('inf')  # Initialize best validation loss\n",
    "boundary_iterations = 2  # Number of iterations for boundary mask computation\n",
    "\n",
    "# Criterion\n",
    "criterion = BoundaryAwareBCELossFineTuning(lambda_weight=3.0)\n",
    "\n",
    "\n",
    "# Early Stopping\n",
    "patience = 3\n",
    "counter = 0\n",
    "early_stop = False\n",
    "\n",
    "# Fine-tuning parameters\n",
    "num_finetune_epochs = 2\n",
    "power = 0.9  # for poly LR schedule\n",
    "# Calculate max iterations for poly schedule\n",
    "max_iter = num_finetune_epochs * len(train_loader_lostandfound)\n",
    "current_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning loop\n",
    "for epoch in range(0, num_finetune_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, masks, original_mask in tqdm(train_loader_lostandfound, desc=f\"Fine-tune Epoch {epoch+1}\"):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "\n",
    "        loss = criterion(preds, masks, boundary_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Poly learning rate update\n",
    "        current_iter += 1\n",
    "        lr = fine_tune_lr * (1 - current_iter / max_iter) ** power\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader_lostandfound)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, original_mask in val_loader_lostandfound:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            boundary_masks = get_boundary_mask_batch(original_mask, iterations=boundary_iterations).detach().to(device)\n",
    "            loss = criterion(preds, masks, boundary_masks)\n",
    "            val_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader_lostandfound)\n",
    "    print(f\"[Fine-tune Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'current_iter': current_iter,\n",
    "        }, 'weights/new_weights/fine_tuned_model_boundary_5_lambda_3.0_boundaryft_2_epochsft_2.pth')\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
    "            early_stop = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW METHOD ####\n",
    "\n",
    "calibration_scores = []\n",
    "\n",
    "# Define the macro-classes considered as known objects\n",
    "KNOWN_OBJECT_CLASSES = [2, 3, 6]  # human, vehicle, pole\n",
    "\n",
    "model = MultiLabelDeepLabV3(n_classes=8).to(device)\n",
    "\n",
    "checkpoint = torch.load('inserire peso', map_location = device) #inserire peso\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "for images_batch, labels_batch, _ in cal_loader: \n",
    "    images_batch = images_batch.to(device)\n",
    "    labels_batch = labels_batch.to(device)  # shape: (B, C, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(images_batch)  # shape: (B, 8, H, W), sigmoid\n",
    "        nonconformity_scores = nonconformity_score(output)\n",
    "\n",
    "    for b in range(images_batch.size(0)):\n",
    "        lbl = labels_batch[b]  # (C, H, W)\n",
    "        non_conf_score = nonconformity_scores[b]  # (H, W)\n",
    "\n",
    "        # Build a binary mask of pixels that belong to known object classes\n",
    "        mask = lbl[KNOWN_OBJECT_CLASSES].any(dim=0) # (H, W) true for pixels belonging to known objects\n",
    "\n",
    "        # Apply mask and extract corresponding unknown objectness scores\n",
    "        selected_scores = non_conf_score[mask]  # 1D tensor\n",
    "\n",
    "        calibration_scores.extend(selected_scores.cpu().numpy())\n",
    "\n",
    "# Save sorted calibration scores\n",
    "calibration_scores = np.sort(np.array(calibration_scores))\n",
    "np.save(\"calibration_scores.npy\", calibration_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the heatmap superposed on the image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Recreate the model architecture\n",
    "model = MultiLabelDeepLabV3(n_classes=8)\n",
    "\n",
    "model.to(device)  # move to GPU or CPU as appropriate\n",
    "\n",
    "resized_height = 512\n",
    "resized_width = 1024\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((resized_height, resized_width)),  # Resize to half the original size\n",
    "    T.ToTensor(),  # converts in [0, 1], shape [3, H, W]\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "path = \"../../obstacles12_rocks5.png\"\n",
    "\n",
    "with Image.open(path) as img:\n",
    "    img = img.convert(\"RGB\")  # Ensure it's in RGB mode\n",
    "    img = img.resize((1024, 512), resample=Image.BILINEAR)  # Resize to match model input size\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img)  # Add batch dimension and move to device\n",
    "\n",
    "\n",
    "weight_name = \"fine_tuned_model_boundary_5_lambda_3.0_epochs_5_boundaryft_1_epochsft_3.pth\"  # Change this to the desired model name\n",
    "\n",
    "checkpoint = torch.load('weights/new_weights/' + weight_name, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "#img = denormalize(img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    uos = unknown_objectness_score(model(img_tensor.unsqueeze(0).to(device)))[0]\n",
    "    # plt.imshow(uos.cpu().numpy(), cmap='hot')\n",
    "    # plt.title(f\"UOS for val_set[{idx}]\")\n",
    "    # plt.colorbar()\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "img_test = denormalize(img_tensor)\n",
    "uos_heatmap(img_test, uos, threshold=0.6, alpha_val=0.5)\n",
    "\n",
    "threshold = 0.2024 # Set your conformal threshold here\n",
    "visualize_uos_with_conformal(model, img_test, device, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks, original_mask = next(iter(val_loader))  # Get a batch from the validation set\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "preds = (model(imgs))  # if output is logits\n",
    "preds = preds.detach().cpu()\n",
    "masks = masks.cpu()\n",
    "\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy(preds, masks))\n",
    "print(\"Mean IoU:\", mean_iou(preds, masks))\n",
    "print(\"Dice per class:\", dice_score(preds, masks))\n",
    "prec, rec = precision_recall(preds, masks)\n",
    "print(\"Precision per class:\", prec)\n",
    "print(\"Recall per class:\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BENCHMARK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
